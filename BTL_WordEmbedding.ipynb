{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_3fBgknRTknw",
        "zz71h4bc-Ghp",
        "hNEF7Qha-Djw",
        "yFHm9Eqxat3C",
        "ahcC6qria0gy",
        "I610VhrzoyXK",
        "SOjRyoTGvLlN",
        "vRDOFwrHvQ_X",
        "Kdx-G_JxvV1B",
        "9rQzium4vkvo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Team members:\n",
        "## NgÃ´ HoÃ i TÃº - 2570536\n",
        "## Nguyá»…n HoÃ ng KiÃªn - 2570435\n",
        "## Tráº§n Quá»‘c Viá»‡t - 2570154\n",
        "## Huá»³nh Äá»©c NhÃ¢m - 2570276"
      ],
      "metadata": {
        "id": "wynwDYiD-Qf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction to Word Embeddings\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "In the field of **Natural Language Processing (NLP)**, the fundamental challenge lies in translating human languageâ€”which consists of discrete, symbolic, and unstructured dataâ€”into a format that machine learning algorithms can process.\n",
        "\n",
        "Computers operate on numerical data, specifically vectors and matrices. Therefore, we need a robust mapping function to transform the vocabulary of a language into a vector space.\n",
        "\n",
        "### The Semantic Gap\n",
        "The core problem is not merely assigning numbers to words, but preserving **semantic meaning**.\n",
        "* **Traditional approaches** treated words as atomic symbols with no inherent relationship.\n",
        "* **The Result:** The system could not understand that \"laptop\" and \"notebook\" are related.\n",
        "* **The Goal:** Word Embedding bridges this gap by creating a continuous vector space where geometric proximity reflects semantic similarity.\n",
        "\n",
        "## Applications\n",
        "\n",
        "The advent of dense vector representations has revolutionized the AI landscape. Key applications include:\n",
        "\n",
        "* **Semantic Search:** Modern search engines use embeddings to understand user intent beyond exact keyword matching (e.g., matching \"cheap hotel\" with \"budget hostel\").\n",
        "* **Machine Translation:** Neural Machine Translation (NMT) systems rely on embeddings to map the semantic space of the source language to the target language.\n",
        "* **Recommendation Systems:** Variants like *Item2Vec* treat user sessions as \"sentences\" and items as \"words,\" allowing platforms to recommend items that are mathematically similar.\n",
        "* **Sentiment Analysis:** By capturing the semantic orientation of words, models can better detect sarcasm, nuance, and polarity.\n",
        "\n",
        "## Solving CS Problems\n",
        "\n",
        "Word2Vec provided elegant solutions to classic Computer Science challenges:\n",
        "\n",
        "1.  **The Curse of Dimensionality:** Traditional models required dimensions equal to the vocabulary size ($100,000+$). Word embeddings perform non-linear dimensionality reduction, compressing this to $\\approx 300$ dimensions.\n",
        "2.  **Unsupervised Feature Extraction:** Instead of manual feature engineering, embeddings use **Self-Supervised Learning** to learn from raw text.\n",
        "3.  **Analogical Reasoning:** The system solves analogies purely through vector arithmetic:\n",
        "    $$Vector(\\text{King}) - Vector(\\text{Man}) + Vector(\\text{Woman}) \\approx Vector(\\text{Queen})$$"
      ],
      "metadata": {
        "id": "06Fc6lNt6-Nk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Background: The One-Hot Encoding Approach\n",
        "\n",
        "## Definition and Mechanism\n",
        "\n",
        "Before distributed representations, the standard method was **One-Hot Encoding**.\n",
        "\n",
        "A One-Hot vector represents a word as a vector $v \\in \\mathbb{R}^{|V|}$ where only one element is $1$ and all others are $0$.\n",
        "For a vocabulary $V = [\\text{apple}, \\text{banana}, \\text{cat}]$:\n",
        "\n",
        "$$w_{\\text{apple}} = [1, 0, 0]$$\n",
        "$$w_{\\text{banana}} = [0, 1, 0]$$\n",
        "$$w_{\\text{cat}} = [0, 0, 1]$$\n",
        "\n",
        "## Why is One-Hot a bad choice?\n",
        "\n",
        "While simple, One-Hot encoding suffers from critical flaws:\n",
        "\n",
        "* **High Dimensionality:** For 100,000 words, every vector length is 100,000. This is computationally wasteful.\n",
        "* **Data Sparsity:** Vectors are mostly zeros, making gradient updates inefficient in neural networks.\n",
        "* **Lack of Semantic Similarity (Orthogonality):** In One-Hot space, every word is orthogonal ($90^\\circ$) to every other word.The dot product of any two distinct words is always zero:\n",
        "\n",
        "    $$w_{\\text{hotel}}^T \\cdot w_{\\text{motel}} = 0$$\n",
        "    The model learns **zero** information about the relationship between synonyms."
      ],
      "metadata": {
        "id": "45wmqODK87dT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Transition to Probability-Based Representations\n",
        "\n",
        "To overcome One-Hot limitations, we shift from **counting** to **predicting**.\n",
        "\n",
        "We utilize **Self-Supervised Learning** based on the **Distributional Hypothesis**:\n",
        "> *\"A word is characterized by the company it keeps.\"*\n",
        "\n",
        "By analyzing the probability of a word appearing within a specific context, **Word2Vec** (CBOW/Skip-gram) learns dense vectors where similar words have similar probability distributions.\n",
        "\n",
        "---\n",
        "\n",
        "# 4. Geometric and Linear Algebraic Operations\n",
        "\n",
        "Once we have vectors, we need mathematical tools to measure relationships.\n",
        "\n",
        "## Dot Product\n",
        "\n",
        "### Definition\n",
        "The dot product is a basic operation to determine how \"aligned\" two vectors are. For two word vectors $\\mathbf{a}$ and $\\mathbf{b}$ in an $n$-dimensional space:\n",
        "\n",
        "$$\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{n} a_i b_i = a_1 b_1 + a_2 b_2 + \\dots + a_n b_n$$\n",
        "\n",
        "### Breaking Down the Formula\n",
        "* $\\mathbf{a}, \\mathbf{b}$: The two vectors representing two words.\n",
        "* $n$: The **number of dimensions**. In our example below $n=2$, but in Word2Vec, $n$ is usually 300.\n",
        "* $i$: The index running from 1 to $n$.\n",
        "* $a_i, b_i$: The specific value at position $i$ in each vector.\n",
        "* $\\sum$: The Summation symbol (multiply corresponding pairs, then add them all up).\n",
        "\n",
        "Geometrically, it relates to the angle $\\theta$:\n",
        "$$a \\cdot b = \\|a\\| \\|b\\| \\cos(\\theta)$$\n",
        "\n",
        "### Step-by-Step Example\n",
        "Let's calculate the Dot Product for two 2D vectors:\n",
        "* **Vector A (\"Phone\"):** $\\mathbf{a} = [1, 3]$\n",
        "* **Vector B (\"Smartphone\"):** $\\mathbf{b} = [4, 5]$\n",
        "\n",
        "**Calculation:**\n",
        "1.  **Pair 1:** $1 \\times 4 = 4$\n",
        "2.  **Pair 2:** $3 \\times 5 = 15$\n",
        "3.  **Sum:** $4 + 15 = 19$\n",
        "\n",
        "**Result:** The Dot Product is **19**.\n",
        "*(Note: This number indicates alignment but is affected by the magnitude of the vectors, making it unreliable for semantic comparison on its own).*\n",
        "\n",
        "---\n",
        "\n",
        "## Cosine Similarity\n",
        "\n",
        "### Definition\n",
        "To overcome the magnitude dependency of the dot product, we use **Cosine Similarity**. This is the gold standard in NLP for determining semantic similarity. It measures the cosine of the angle $\\theta$ between two vectors.\n",
        "\n",
        "$$\\text{Cosine Similarity} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|}$$\n",
        "\n",
        "### Breaking Down the Formula\n",
        "* **Numerator ($\\mathbf{a} \\cdot \\mathbf{b}$):** The Dot Product we calculated above.\n",
        "* **Denominator ($\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|$):** The product of the lengths (Euclidean Norms) of the two vectors. This acts as a normalizer to remove the influence of vector magnitude.\n",
        "\n",
        "### Step-by-Step Example (Continued)\n",
        "\n",
        "**Step 1: Calculate Vector Magnitudes (Norms)**\n",
        "The formula for magnitude is $\\|\\mathbf{x}\\| = \\sqrt{\\sum x_i^2}$ (Pythagorean theorem).\n",
        "\n",
        "* **Magnitude of A:**\n",
        "    $\\|\\mathbf{a}\\| = \\sqrt{1^2 + 3^2} = \\sqrt{1 + 9} = \\sqrt{10} \\approx \\mathbf{3.16}$\n",
        "* **Magnitude of B:**\n",
        "    $\\|\\mathbf{b}\\| = \\sqrt{4^2 + 5^2} = \\sqrt{16 + 25} = \\sqrt{41} \\approx \\mathbf{6.40}$\n",
        "\n",
        "**Step 2: Calculate Cosine Similarity**\n",
        "Now we plug the values into the main formula:\n",
        "\n",
        "$$\\text{similarity} = \\frac{19}{3.16 \\times 6.40}$$\n",
        "\n",
        "$$\\text{similarity} = \\frac{19}{20.224} \\approx \\mathbf{0.939}$$\n",
        "\n",
        "### Interpretation\n",
        "* **Result 0.939:** Since the maximum value is 1.0, a score of 0.939 is extremely high.\n",
        "* **Conclusion:** The vectors for \"Phone\" and \"Smartphone\" point in almost the exact same direction. The machine successfully recognizes them as **synonyms**.\n",
        "\n",
        "| Value Range | Meaning | Example |\n",
        "| :--- | :--- | :--- |\n",
        "| $\\approx 1$ | Synonyms (Same direction) | \"Phone\" vs \"Mobile\" |\n",
        "| $\\approx 0$ | Unrelated (Orthogonal/$90^\\circ$) | \"Phone\" vs \"Banana\" |\n",
        "| $\\approx -1$ | Antonyms (Opposite direction) | \"Good\" vs \"Bad\" |"
      ],
      "metadata": {
        "id": "-HLOc9pd8_UF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-Supervised Word to Vector Method"
      ],
      "metadata": {
        "id": "kAkVhrRPF2Me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "_3fBgknRTknw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To beyond the limitations of One-Hot method, there was an approach based on Self-Supervised learning with two architectures: The Skip-Gram model and the Continuous Bag-of-Words (CBOW) model [(Mikolov et al., 2013)](https://arxiv.org/pdf/1301.3781).\n",
        "\n",
        "Following the paper, the authors agreed that the word vectors capture both closeness of similar words and more complex, multi-layered relationships between words (syntactic, morphological, etc). Surprisingly, they found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example:\n",
        "\n",
        "$$vector(''\\text{King}'') - vector(''\\text{Man}'') + vector(''\\text{Woman}'') \\,\\,\\sim\\,\\, vector(''\\text{Queen}'')$$\n",
        "\n",
        "> **They propose two new model architectures to maximize accuracy of these vector operations and preserve the linear regularities among words while minimizing computational complexity by using simple model instead of full neural network (which contains non-linear hidden layers)**\n",
        "\n",
        "Following their method, they do not treat words as atomic units â€“ there is no notion of similarity between words. Instead, both CBOW and skip-gram learn distributed representations by leveraging the local context in which words appear. The CBOW model predicts a target word based on its surrounding words, effectively aggregating contextual information to infer meaning. Conversely, the skip-gram model predicts the surrounding context words given a single target word, aiming to learn word vectors that are informative enough to generate their typical neighbors in text. Together, these two approaches capture semantic and syntactic regularities by exploiting patterns that naturally occur in large corpora.\n",
        "\n",
        "This approach is grounded in the use of conditional probability, where models learn word representations by estimating the likelihood of a target word given its context or vice versa. Through these probability-based predictions, the embeddings capture meaningful semantic relationships directly from unlabeled text in the corpus - the reason why this method is self-supervised.\n",
        "\n",
        "![Illustration for center word vÃ  context](https://vnmap-backend.inut.vn/uploads/image_1_1ed7d3bf90.png)\n",
        "\n",
        "![Overview of two models](https://vnmap-backend.inut.vn/uploads/image_2_8ca2568222.png)"
      ],
      "metadata": {
        "id": "t9ZMBcvl-VZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Continuous Bag of Words Model"
      ],
      "metadata": {
        "id": "zz71h4bc-Ghp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Dual-Vector of a Word"
      ],
      "metadata": {
        "id": "9PYw0ao2VkZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a dictionary of words calling $V$. Suppose, each word at index $i$ is represented by a vector $v_i \\in \\mathbb{R}^N$ (This is also our desired outcome).\n",
        "\n",
        "Let's start with an idea for an application that: We build a model $M$ (not CBOW model) to help us fill in a blank in a sentence. For example: \"we are ___ friends forever\", the model will predict the blank is \"good\". Overall, our goal is predicting a word based on its context (surrounding words):\n",
        "$$M(v_{we},\\, v_{are},\\, v_{friends},\\, v_{forever}) = \\text{''good''}$$\n",
        "\n",
        "Crucially, the model discards the order of the context words. Then, we define the representation of the context as the **average** of the input vectors of the surrounding words:\n",
        "$$\\bar{v} = \\frac{v_{we}+v_{are}+v_{friends}+v_{forever}}{4}$$\n",
        "\n",
        "Model $M$ maximizes the following probability:\n",
        "$$p(\\text{\"good\"} \\,|\\, \\bar{v}\\,\\,\\,)$$\n",
        "\n",
        "Instead of represent the output classes of model $M$ as strings like \"good\", for more mathematical, we define $u_{good} \\in \\mathbb{R}^N$ which is a vector has the same dimension with $v_i$, as the output class of word \"good\". Because all the words in the dictionary $V$ are also all the classes of model $M$, each word at index $i$ of $W$ has two vectors: $v_i$ for its features at input and $u_i$ for representing it at output (Both $ \\in \\mathbb{R}^N$).\n",
        "\n",
        "Thus:\n",
        "$$p(\\text{\"good\"} \\,|\\, \\bar{v}\\,\\,\\,) = p(u_{good} \\,|\\, \\bar{v}\\,\\,\\,)$$\n",
        "\n",
        "Suppose we work with the sequence of $2m+1$ words including a blank and $2m$ surrounding context words ($m$ context words on the left and $m$ context words on the right). Therefore, we aim to predict a center word based on its $2m$ context words by maximizing the probability:\n",
        "$$p( u_c \\mid v_{o_1},v_{o_2},...,v_{o_{2m}} ) = p(u_c \\mid \\bar{v_o})$$\n",
        "\n",
        "Summary, we have two unknown vectors: $v$ and $u$. Let's continue to build ideas."
      ],
      "metadata": {
        "id": "wzxpxOr0KMpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use One-Hot Encoded vector as Output layer"
      ],
      "metadata": {
        "id": "QjqpzQpAoX9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the model $M$, we define a new simple model $M'$ containing only two layers: Input layer is $x \\in \\mathbb{R}^N$ for context $\\bar{v_o}$ and Output layer is One-hot encoded vector $y \\in \\mathbb{R}^{{|V|}}$ of the center word $w_c$. The output layer is handled with the softmax function. With $M'$, we have only one kind of unknown vector: $v$, at input.\n",
        "\n",
        "![](https://vnmap-backend.inut.vn/uploads/image_3_6b5f287ca8.png)\n",
        "\n",
        "We have:\n",
        "$$y = W'^{\\,T}.x$$\n",
        "\n",
        "with $W'$ is the weights of the model $M'$.\n",
        "\n",
        "Following the softmax of this simple neural net, we have:\n",
        "\n",
        "$$p(word_{\\,i} \\mid x = \\bar{v_o}) = y_i = \\frac{exp(W'^{\\,T}_{(\\cdot,\\,i)}\\cdot x)}{\\Sigma_{j=1}^{|V|}\\,exp(W'^{\\,T}_{(\\cdot,\\,j)}\\cdot x)}$$\n",
        "\n",
        "where: $y_i$ is the output of the i-th unit in the output layer, $W'_{(\\cdot,\\,i)} \\in \\mathbb{R}^{N\\,x\\,1}$ is the i-th column of the matrix $W'$\n",
        "\n",
        "Call $c$ is the true index of the center word, the target of training is maximizing the probability:\n",
        "$$p(word_{c} \\mid x = \\bar{v_o}) \\,\\, \\sim \\,\\, p(u_c \\mid \\bar{v_o})$$\n",
        "with $u_c$ is the one in the Dual-vectors representing the $word_c$ (mentioned in the last section)\n",
        "\n",
        "The maxium $p(word_{c} \\mid x = \\bar{v_o})$ is related with the weights column $W'_{(\\cdot,\\,c)}$ so we assign $u_c := W'_{(\\cdot,\\,c)}$ or $u_i := W'_{(\\cdot,\\,i)}$\n",
        "\n",
        "Therefore:\n",
        "$$p(word_{c} \\mid x = \\bar{v_o}) = y_c = \\frac{exp(u^T_c \\cdot \\bar{v_o})}{\\Sigma_{j=1}^{|V|}\\,exp(u^T_j \\cdot \\bar{v_o})}$$"
      ],
      "metadata": {
        "id": "ONCOgymkVkZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use One-Hot Encoded vector as Input layer"
      ],
      "metadata": {
        "id": "h1yPq92fVkZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the model $M$, we define a new simple model $M''$ containing only two layers: Input layer is multi-inputs which are One-hot encoded vectors $x_i \\in \\mathbb{R}^{{|V|}}$ for context $w_1, w_2,...,w_{2m}$ and Output layer is $y \\in \\mathbb{R}^N$ of the center word $w_c$. Note that: the output vector is not the $u$ vector of the center word. This model show the idea for how to use multi context inputs.\n",
        "\n",
        "![](https://vnmap-backend.inut.vn/uploads/image_5_38a3164274.png)\n",
        "\n",
        "We take the average of the vectors of the input context words, and use the product of the input. Thus:\n",
        "\n",
        "$$y = \\frac{1}{2m}\\, W^T \\cdot (x_1 + x_2 + \\dots + x_{2m})$$\n",
        "with $W$ is the weights of the model $M''$ shared for all inputs.\n",
        "\n",
        "Given a context, assuming $x_i^k = 1$ and $x_i^{k'} \\ne 0$ for $k \\ne k'$, we set:\n",
        "$$W^T \\cdot x_i = W^T_{(k_i,\\cdot)} := v^{T}_{w_i}$$\n",
        "\n",
        "We can assign $W_{(k_i,\\cdot)} := v_{w_i}$, because the output vector is contributed by $x_i \\sim W_{(k_i,\\cdot)}$\n",
        "\n",
        "Therefore:\n",
        "$$y = \\frac{1}{2m}\\,(v^T_{w_1}+v^T_{w_2}+...+v^T_{w_{2m}}) = \\frac{1}{2m}\\,(v_{1}+v_{2}+...+v_{2m})^T$$"
      ],
      "metadata": {
        "id": "EMSyTt_nVkZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full CBOW Model"
      ],
      "metadata": {
        "id": "jftDkbfRVkZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine all three above ideas, we get the full CBOW architecture as:\n",
        "![](https://vnmap-backend.inut.vn/uploads/image_6_a0159dcddb.png)\n",
        "\n",
        "Modeled by:\n",
        "$$p(w_c \\mid ð’²_o) = \\frac{exp(\\frac{1}{2m}u_c^T (v_{o_1}+v_{o_2}+\\cdots +v_{o_{2m}}))}{\\Sigma_{i=1}^{|V|}\\, exp(\\frac{1}{2m}u_i^T (v_{o_1}+v_{o_2}+\\cdots +v_{o_{2m}}))} = \\frac{exp(u_c^T \\cdot \\bar{v_o})}{\\Sigma_{i=1}^{|V|}\\, exp(u_i^T \\cdot \\bar{v_o})}$$\n",
        "\n",
        "Given a text sequence of length $T$ for training, $w^{(t)}$ is the word at index $t$ of the sequence. For context window size $m$, the likelihood function of the continuous bag of words model is the probability of generating all center words given their context words:\n",
        "$$\\prod_{t=1}^{T} p\\!\\left(w^{(t)} \\,\\middle|\\, w^{(t-m)}, \\ldots, w^{(t-1)},\\, w^{(t+1)}, \\ldots, w^{(t+m)}\\right).$$\n",
        "\n",
        "Following the Maximum likelihood estimation (MLE), we will train the full CBOW model by maximizing this likelihood function."
      ],
      "metadata": {
        "id": "zBc83wxwVkZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training CBOW model"
      ],
      "metadata": {
        "id": "OPXXhcAEVkZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training objective (for one training sample) is to maximize $p(w_c \\mid ð’²_o)$, the conditional\n",
        "probability of observing the actual output word $w_c$ (with index i) given the input context word $ð’²_o$ with regard to the weights. However, in machine learning, we need to minimize the Loss function. Then, we need to build the loss function $L$ for a training pair.\n",
        "\n",
        "We have:\n",
        "$$max\\, p(w_c \\mid ð’²_o) â‡” max\\, y_i â‡” max\\, log\\,y_i = log(exp(u_c^T \\cdot \\bar{v_o})) - log(\\Sigma_{i=1}^{|V|}\\, exp(u_i^T \\cdot \\bar{v_o})) = u_c^T \\cdot \\bar{v_o} - log(\\Sigma_{i=1}^{|V|}\\, exp(u_i^T \\cdot \\bar{v_o}))\\,:= -L$$\n",
        "\n",
        "Then, we assign:\n",
        "$$Å = -\\Sigma_{t=1}^{T}\\,log\\,p\\!\\left(w^{(t)} \\,\\middle|\\, w^{(t-m)}, \\ldots, w^{(t-1)},\\, w^{(t+1)}, \\ldots, w^{(t+m)}\\right) = \\Sigma_{t=1}^{T}L_t\\,$$\n",
        "\n",
        "\n",
        "Through differentiation, we can obtain its gradient with respect to any context word vector $v_{o_i} (i=1,...,2m)$ as:\n",
        "$$\\frac{âˆ‚ L}{\\partial\\,v_{o_i}}=\\frac{\\partial\\,log\\,p(w_c \\mid ð’²_o)}{\\partial\\,v_{o_i}} = \\frac{\\partial\\,(log(\\Sigma_{i=1}^{|V|}\\, exp(u_i^T \\cdot \\frac{1}{2m}(v_1+...+v_{2m}))) - u_c^T\\cdot\\ \\frac{1}{2m}(v_1+..+v_{o_i}+..+v_{2m}))}{\\partial\\,v_{o_i}} = \\frac{(\\Sigma_{i=1}^{|V|}\\, exp(u_i^T \\cdot \\frac{1}{2m}(v_1+...+v_{2m})))'}{\\Sigma_{j=1}^{|V|}\\, exp(u_j^T \\cdot \\frac{1}{2m}(v_1+...+v_{2m}))}-\\frac{1}{2m}u_c^T = \\frac{\\Sigma_{i=1}^{|V|}\\,\\left( exp(u_i^T \\cdot \\frac{1}{2m}(v_1+...+v_{2m})) \\cdot (u_i^T \\cdot \\frac{1}{2m}(v_1+...+v_{2m}))'\\right)}{\\Sigma_{j=1}^{|V|}\\, exp(u_j^T \\cdot \\frac{1}{2m}(v_1+...+v_{2m}))}-\\frac{1}{2m}u_c^T = \\frac{\\Sigma_{i=1}^{|V|}\\,\\left( exp(u_i^T \\cdot \\frac{1}{2m}(v_1+...+v_{2m})) \\cdot (u_i^T \\cdot \\frac{1}{2m})\\right)}{\\Sigma_{j=1}^{|V|}\\, exp(u_j^T \\cdot \\frac{1}{2m}(v_1+...+v_{2m}))}-\\frac{1}{2m}u_c^T = \\frac{1}{2m}\\left(\\frac{\\Sigma_{i=1}^{|V|}\\,\\left( exp(u_i^T \\cdot \\frac{1}{2m}(v_1+...+v_{2m})) \\cdot u_i^T\\right)}{\\Sigma_{j=1}^{|V|}\\, exp(u_j^T \\cdot \\frac{1}{2m}(v_1+...+v_{2m}))}-u_c^T\\right) = \\frac{1}{2m}\\left(\\Sigma_{i=1}^{|V|}\\,\\frac{exp(u_i^T \\cdot \\frac{1}{2m}(v_1+...+v_{2m}) \\cdot u_i^T}{\\Sigma_{j=1}^{|V|}\\, exp(u_j^T \\cdot \\frac{1}{2m}(v_1+...+v_{2m}))}-u_c^T\\right) = \\frac{1}{2m}\\left(\\Sigma_{i=1}^{|V|}\\,\\frac{exp(u_i^T \\cdot \\bar{v_o}) \\cdot u_i^T}{\\Sigma_{j=1}^{|V|}\\, exp(u_j^T \\cdot \\bar{v_o})}-u_c^T\\right)\n",
        "= \\frac{1}{2m}\\left( \\Sigma_{i=1}^{|V|}\\,p(w_i \\mid ð’²_o).u_i^T - u_c \\right)$$"
      ],
      "metadata": {
        "id": "2PxQ2SXoVkZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Skip-Gram Model"
      ],
      "metadata": {
        "id": "hNEF7Qha-Djw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reversely with CBOW mode, the skip-gram model assumes that a center word can be used to generate its surrounding words (context) in a text sequence. More precisely, we use each current word as an input to a log-linear classifier with continuous projection layer (the hidden layer), and predict words within a certain range before and after the current word. We can represent as following conditional probability:\n",
        "$$p(\\text{\"we\"},\\text{\"are\"},\\text{\"friends\"},\\text{\"forever\"} \\mid \\text{\"good\"}) \\sim  p(u_\\text{we},u_\\text{are},u_\\text{friends},u_\\text{forever} \\mid v_\\text{good}) \\,\\,\\text{on model $M$}$$\n",
        "\n",
        "Therefore, for any word with index $i$ in the dictionary, we call vector $v_i \\in \\mathbb{R}^N$ as its representation in center role and vector $u_i \\in \\mathbb{R}^N$ as its representation in context role."
      ],
      "metadata": {
        "id": "KvYF69p4Yerf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why CBOW and Skip-Gram are consistent"
      ],
      "metadata": {
        "id": "yFHm9Eqxat3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the conditional probability on model $M$, we have:\n",
        "$$P(A|B).P(B) = P(B|A).P(A)$$\n",
        "or:\n",
        "$$P(center = C \\mid context = O).P(context = O) = P(context = O \\mid center = C).P(center = C)$$\n",
        "\n",
        "Do not mind about $P(context = O)$ and $P(center = C)$, maximizing the $P(context = O \\mid center = C)$ is consistent with the goal of model $M$ - maximizing the $P(center = C \\mid context = O)$."
      ],
      "metadata": {
        "id": "WXia8P2aXfCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume that the context words are independently generated given the center word (i.e., conditional independence). In this case, the skip-gram conditional probability can be rewritten as:\n",
        "\n",
        "$$\n",
        "p(''\\text{we}'' \\,|\\, ''\\text{good}'') \\,.\\,\n",
        "p(''\\text{are}'' \\,|\\, ''\\text{good}'') \\,.\\,\n",
        "p(''\\text{friends}'' \\,|\\, ''\\text{good}'') \\,.\\,\n",
        "p(''\\text{forever}'' \\,|\\, ''\\text{good}'')\n",
        "$$\n",
        "\n",
        "Therefore, we can say that: Instead of predicting the current word based on the context, the Skip-Gram model tries to maximize classification of a word based on another word in the same sentence:\n",
        "$$p(w_o \\mid w_c)$$\n",
        "with $w_o$ is the word surrounding the center word $w_c$ in a window size $m$"
      ],
      "metadata": {
        "id": "iA4sppH-SUzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skip-Gram architecture"
      ],
      "metadata": {
        "id": "ahcC6qria0gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on built ideas in previous sections about model $M, M', M''$, we explore the Skip-Gram model as below figure:\n",
        "![](https://vnmap-backend.inut.vn/uploads/image_7_f8ec2e1d6c.png)\n",
        "with $x$ is the One-hot encoded vector of the center word, $y_{i} \\,i \\in [1, C=2m]$ are the One-hot encoded vectors of the context words. $W_{V x\\, N}$ is the weights matrix whose each row at index $i$ is the $v_i$ vector of word $i$ in dictionary. $W'_{N\\,x \\, V}$ is the weights matrix whose each column at index $j$ is the $u_j$ vector of word $j$ in dictionary.\n",
        "\n"
      ],
      "metadata": {
        "id": "7Y1ZtJ1ba28B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to CBOW, at output layer with softmax, we have $p(w_o \\mid w_c)$ formula as below, where $w_o$ is the word at index $o$ and $w_c$ is the word at index $c$ in the dictionary.\n",
        "$$\n",
        "h = W_{(c,\\cdot)}^T = v_{c}^T\n",
        "$$\n",
        "$$\n",
        "p(w_o \\mid w_c) = y_{ij}^{o} = softmax(W'^T\\cdot h)^o = \\frac{exp(u_o^Tv_c)}{\\Sigma_{i=1}^{|V|}\\,exp(u_i^Tv_c)}\n",
        "$$\n",
        "with $y^o$ is the o-th element in the output vector $y$ and $y^o = u_o^T\\cdot h$ cause of $u_o$ is the o-th column of the $W'$\n",
        "\n",
        "Given a text sequence of length $T$, where the word at time step\n",
        " is denoted as $w^{(t)}$. Assume that context words are independently generated given any center word. For context window size $m$, the likelihood function of the skip-gram model is the probability of generating all context words given any center word:\n",
        " $$\n",
        " \\prod^T_{t=1}\\,\\prod_{-m \\le j \\le m,j \\ne 0}\\,p(w^{t+j} \\mid w^{(t)})\n",
        " $$\n",
        " where any time step that is less than $1$ or greater than $T$ can be omitted.\n",
        "\n",
        " Following the Maximum likelihood estimation (MLE), we will train the full Skip-gram model by maximizing this likelihood function."
      ],
      "metadata": {
        "id": "D3d5jAOKbcIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Skip-gram model"
      ],
      "metadata": {
        "id": "I610VhrzoyXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Skip-gram architecture, each training sample consists of a **center word** $w_c$ with index $c$ and one of its **context words** $w_o$ with index $o$. The model aims to maximize the conditional probability  \n",
        "$$\n",
        "p(w_o \\mid w_c)\n",
        "$$\n",
        "with respect to the model parameters. To align with the standard machine-learning practice of minimizing an objective, we construct the loss function accordingly.\n",
        "\n",
        "The Skip-gram model predicts each context word independently given the center word. The probability of observing a specific context word $w_o$ is:\n",
        "$$\n",
        "p(w_o \\mid w_c)\n",
        "= \\frac{\\exp(u_o^{T} v_c)}\n",
        "       {\\sum_{i=1}^{|V|} \\exp(u_i^{T} v_c)}\n",
        "$$\n",
        "\n",
        "Maximizing this probability is equivalent to maximizing its logarithm:\n",
        "\n",
        "$$\n",
        "\\max\\, p(w_o \\mid w_c)\n",
        "\\;\\;\\Longleftrightarrow\\;\\;\n",
        "\\max\\, \\log p(w_o \\mid w_c)\n",
        "$$\n",
        "\n",
        "Then:\n",
        "\n",
        "$$\n",
        "\\log p(w_o \\mid w_c)\n",
        "= u_o^{T} v_c - \\log\\!\\left(\\sum_{i=1}^{|V|} \\exp(u_i^{T} v_c)\\right)\n",
        "\\;:=\\; -L\n",
        "$$\n",
        "\n",
        "Thus, the loss for one centerâ€“context pair is:\n",
        "\n",
        "$$\n",
        "L = \\log\\!\\left(\\sum_{i=1}^{|V|} \\exp(u_i^{T} v_c)\\right) - u_o^{T} v_c\n",
        "$$"
      ],
      "metadata": {
        "id": "Q8KT48KTo7mI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a corpus sequence $w^{(1)}, w^{(2)}, \\ldots, w^{(T)}$, the Skip-gram objective considers a window of size \\( m \\).  \n",
        "For each center word position \\( t \\), Skip-gram predicts all its context words:\n",
        "\n",
        "$$\n",
        "w^{(t-m)}, \\ldots, w^{(t-1)},\\, w^{(t+1)}, \\ldots, w^{(t+m)}\n",
        "$$\n",
        "\n",
        "The complete loss over the corpus becomes:\n",
        "\n",
        "$$\n",
        "Å = -\\sum_{t=1}^{T}\n",
        "    \\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}}\n",
        "    \\log p\\!\\left(w^{(t+j)} \\mid w^{(t)}\\right)\n",
        "  = \\sum_{t=1}^{T} \\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}} L_{t,j}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "p6gpfrOZrx3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a given center word $w_c$ and one context word $w_o$, the loss is:\n",
        "\n",
        "$$\n",
        "L = \\log\\!\\left(\\sum_{i=1}^{|V|} \\exp(u_i^{T} v_c)\\right) - u_o^{T} v_c\n",
        "$$\n",
        "\n",
        "Differentiating with respect to $v_c$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial v_c}\n",
        "= \\frac{\n",
        "     \\left(\\sum_{i=1}^{|V|} \\exp(u_i^{T} v_c)\\right)'\n",
        "   }{\n",
        "     \\sum_{j=1}^{|V|}\n",
        "       \\exp(u_j^{T} v_c)\n",
        "   }\n",
        "   - u_o\n",
        "= \\frac{\n",
        "     \\sum_{i=1}^{|V|}\n",
        "       \\exp(u_i^{T} v_c) \\cdot (u_i^Tv_c)'\n",
        "   }{\n",
        "     \\sum_{j=1}^{|V|}\n",
        "       \\exp(u_j^{T} v_c)\n",
        "   }\n",
        "   - u_o\n",
        "= \\frac{\n",
        "     \\sum_{i=1}^{|V|}\n",
        "       \\exp(u_i^{T} v_c) \\cdot u_i\n",
        "   }{\n",
        "     \\sum_{j=1}^{|V|}\n",
        "       \\exp(u_j^{T} v_c)\n",
        "   }\n",
        "   - u_o\n",
        "$$\n",
        "\n",
        "Recognizing the softmax probability:\n",
        "$$\n",
        "p(w_i \\mid w_c)\n",
        "= \\frac{\\exp(u_i^{T} v_c)}{\\sum_{j=1}^{|V|} \\exp(u_j^{T} v_c)}\n",
        "$$\n",
        "\n",
        "we obtain:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial v_c}\n",
        "= \\sum_{i=1}^{|V|} p(w_i \\mid w_c)\\, u_i \\;-\\; u_o\n",
        "$$\n",
        "\n",
        "Similarly, differentiating the loss with respect to each output vector $u_i$:\n",
        "- For the true context word \\( w_o \\):\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial u_o}\n",
        "= \\left(p(w_o \\mid w_c) - 1\\right) v_c\n",
        "$$\n",
        "- For every other word $w_i \\neq w_o $:\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial u_i}\n",
        "= p(w_i \\mid w_c)\\, v_c\n",
        "$$"
      ],
      "metadata": {
        "id": "FnxHPFbisMvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D2L Exercises in Word Embedding"
      ],
      "metadata": {
        "id": "SOjRyoTGvLlN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What is the computational complexity for calculating each gradient? What could be the issue if the dictionary size is huge?"
      ],
      "metadata": {
        "id": "vRDOFwrHvQ_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A. Gradient complexity for Skip-gram"
      ],
      "metadata": {
        "id": "IWLtoJg6NAD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a single centerâ€“context pair, the Skip-gram loss is:\n",
        "\n",
        "$$\n",
        "L = \\log\\!\\left(\\sum_{i=1}^{|V|} \\exp(u_i^{T} v_c)\\right) - u_o^{T} v_c\n",
        "$$\n",
        "\n",
        "Its gradients are:\n",
        "\n",
        "- **With respect to the center-word vector**  \n",
        "$$\n",
        "\\frac{\\partial L}{\\partial v_c}\n",
        "= \\sum_{i=1}^{|V|} p(w_i \\mid w_c)\\, u_i - u_o\n",
        "$$\n",
        "\n",
        "- **With respect to the output vectors**\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial u_i}\n",
        "=\n",
        "\\begin{cases}\n",
        "(p(w_o \\mid w_c) - 1)\\, v_c, & i = o \\\\\n",
        "p(w_i \\mid w_c)\\, v_c, & i \\neq o\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Both gradients depend on **all vocabulary words** because the softmax normalizer contains a full sum over \\(|V|\\).\n",
        "\n",
        "**a. Cost of Computing Softmax Probabilities**\n",
        "\n",
        "To compute  \n",
        "$$\n",
        "p(w_i \\mid w_c) = \\frac{\\exp(u_i^{T} v_c)}{\\sum_{j=1}^{|V|} \\exp(u_j^{T} v_c)}\n",
        "$$\n",
        "\n",
        "we must compute:\n",
        "\n",
        "- $u_i^{T} v_c$ for all $i \\in \\{1,\\dots, |V|\\}$\n",
        "- $\\exp$ for all $i$\n",
        "- the normalization sum over all words\n",
        "\n",
        "Thus, computing the softmax distribution costs:\n",
        "\n",
        "$$\n",
        "\\boxed{\\;O(|V|)\\;}\n",
        "$$\n",
        "\n",
        "**b. Cost of Computing Each Gradient**\n",
        "\n",
        "1. **Gradient with respect to the output vectors $u_i$**  \n",
        "   Requires iterating over **every word** in the vocabulary:\n",
        "   $$\n",
        "   \\frac{\\partial L}{\\partial u_i} = p(w_i\\mid w_c) v_c \\quad \\text{for all } i\n",
        "   $$\n",
        "   Complexity:\n",
        "   $$\n",
        "   \\boxed{\\;O(|V|)\\;}\n",
        "   $$\n",
        "\n",
        "2. **Gradient with respect to the input (center) vector $v_c$**  \n",
        "   Requires the weighted sum over all output vectors:\n",
        "   $$\n",
        "   \\sum_{i=1}^{|V|} p(w_i \\mid w_c) u_i\n",
        "   $$\n",
        "   Complexity:\n",
        "   $$\n",
        "   \\boxed{\\;O(|V|)\\;}\n",
        "   $$\n",
        "\n",
        "**Overall complexity per training instance**\n",
        "$$\n",
        "\\boxed{O(|V|)}\n",
        "$$\n",
        "\n",
        "If there are $C = 2m$ context words (Skip-gram predicts multiple outputs), cost becomes:\n",
        "\n",
        "$$\n",
        "\\boxed{O(C\\,|V|)}\n",
        "$$\n",
        "\n",
        "**B. What If the Dictionary Size Is Huge?**\n",
        "\n",
        "Under large $|V|$, the naive Skip-gram training becomes infeasible.\n",
        "\n",
        "**a. Each gradient update becomes extremely slow**\n",
        "\n",
        "Because each training sample must:\n",
        "- loop over \\(|V|\\) words  \n",
        "- compute \\(|V|\\) dot products  \n",
        "- update \\(|V|\\) output vectors\n",
        "\n",
        "Even with optimized vectorization, this is prohibitive.\n",
        "\n",
        "**b. Training on large corpora becomes impossible**\n",
        "\n",
        "A typical corpus has billions of tokenâ€“context pairs.  \n",
        "Multiplying by $|V|$ makes the runtime explode:\n",
        "\n",
        "$$\n",
        "\\text{Time} \\approx \\text{#pairs} \\times |V|\n",
        "$$\n",
        "\n",
        "Impossible for real-world usage.\n",
        "\n",
        "**c. Memory bandwidth issues**\n",
        "\n",
        "Updating all output vectors implies:\n",
        "- loading millions of vectors each iteration  \n",
        "- producing massive memory traffic  \n",
        "- making GPU/CPU pipelines extremely inefficient\n",
        "\n",
        "**d. Numerical instability in the softmax denominator**\n",
        "As $|V|$ grows, the term  \n",
        "$$\n",
        "\\sum_{j=1}^{|V|} \\exp(u_j^T v_c)\n",
        "$$  \n",
        "can become very large, risking overflow/underflow without careful implementation.\n"
      ],
      "metadata": {
        "id": "JKX2wTq55MQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### B. Gradient complexity for CBOW"
      ],
      "metadata": {
        "id": "igKvapNPPjtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the CBOW architecture, multiple **context word vectors** are averaged to predict a single **center word**.  \n",
        "The loss function for one training instance is:\n",
        "\n",
        "$$\n",
        "L = \\log\\!\\left(\\sum_{i=1}^{|V|} \\exp(u_i^{T} \\bar{v}_o)\\right) - u_c^{T}\\bar{v}_o\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\bar{v}_o = \\frac{1}{2m}\\sum_{k=1}^{2m} v_{o_k}\n",
        "$$\n",
        "\n",
        "is the average of the $2m$ context word vectors.\n",
        "\n",
        "**a. Complexity of Softmax Computation**\n",
        "Just like Skip-gram, CBOW must compute:\n",
        "- all dot products $u_i^T \\bar{v}_o$ for every word in the vocabulary;\n",
        "- the exponential for each value;\n",
        "- the normalization term over all \\(|V|\\) words.\n",
        "\n",
        "Thus the softmax cost is:\n",
        "\n",
        "$$\n",
        "\\boxed{O(|V|)}\n",
        "$$\n",
        "\n",
        "**b. Complexity of Gradient with respect to Output Vectors $u_i$**\n",
        "\n",
        "The derivative:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial u_i} =\n",
        "$$\n",
        "$$\n",
        "(p(w_i \\mid ð’²_o) - 1)\\bar{v}_o,\\,\\,\\, i = c\n",
        "$$\n",
        "$$\n",
        "p(w_i \\mid ð’²_o)\\,\\bar{v}_o,\\,\\,\\, i\\neq c\n",
        "$$\n",
        "\n",
        "must be computed **for every vocabulary word**.  \n",
        "Therefore:\n",
        "$$\n",
        "\\boxed{O(|V|)}\n",
        "$$\n",
        "\n",
        "**c. Complexity of Gradient with respect to Context Input Vectors $v_{o_k}$**\n",
        "\n",
        "The derivative for each context vector is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial v_{o_k}}\n",
        "=\\frac{1}{2m}\\left(\\sum_{i=1}^{|V|} p(w_i \\mid ð’²_o) u_i - u_c \\right)\n",
        "$$\n",
        "\n",
        "Key points:\n",
        "- The expensive inner term  \n",
        "  $$\n",
        "  \\sum_{i=1}^{|V|} p(w_i \\mid ð’²_o) u_i\n",
        "  $$  \n",
        "  requires summing over all output vectors â†’ $O(|V|)$.\n",
        "- This term is **shared** by all $2m$ context words.\n",
        "- Each context word then applies a cheap scaling by $1/(2m)$.\n",
        "\n",
        "Therefore:\n",
        "- Cost to compute shared sum: $O(|V|)$\n",
        "- Cost per context vector: $O(1)$  \n",
        "- Total cost for all $2m$ vectors: $O(|V| + 2m)$\n",
        "\n",
        "Since $2m \\ll |V|$ in language tasks:\n",
        "$\n",
        "O(|V|)\n",
        "$\n",
        "\n",
        "$$\n",
        "\\boxed{\\text{Total cost per CBOW training sample: } O(|V|)}\n",
        "$$\n",
        "\n",
        "**d. Issue When Vocabulary Size is Large**\n",
        "When $|V|$ is large (e.g., 100kâ€“1M words):\n",
        "- Each gradient update becomes extremely slow.  \n",
        "- Training on large corpora becomes impractical because each update requires touching **every** output vector.  \n",
        "- Memory bandwidth becomes a bottleneck as millions of vectors must be accessed per update.\n",
        "\n",
        "This is why efficient approximations such as **hierarchical softmax** and **negative sampling** are essential for real-world word2vec training."
      ],
      "metadata": {
        "id": "hx7ugInPNDCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Some fixed phrases in English consist of multiple words, such as â€œnew yorkâ€. How to train their word vectors?"
      ],
      "metadata": {
        "id": "Kdx-G_JxvV1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-word expressions (MWEs) such as â€œnew yorkâ€ should be **merged into single tokens before training**. Standard CBOW/Skip-gram then learns their vectors just like normal words.\n",
        "\n",
        "**a. Detecting Phrases**\n",
        "\n",
        "The algorithm identifies statistically significant bigrams using the score:\n",
        "\n",
        "$$\n",
        "\\text{score}(w_i, w_j)\n",
        "= \\frac{\\text{count}(w_i, w_j) - \\delta}\n",
        "       {\\text{count}(w_i)\\,\\text{count}(w_j)}\n",
        "$$\n",
        "\n",
        "**Parameter meanings:**\n",
        "\n",
        "- **count($w_i,w_j$)**: Number of times the two words appear consecutively in the corpus.\n",
        "\n",
        "- **count($w_i$), count($w_j$)**: Individual word frequencies. Dividing by their product prevents very common words from forming accidental phrases (e.g., â€œof theâ€).\n",
        "\n",
        "- **$\\delta$** (discount threshold): A constant used to reduce the score of high-frequency but non-meaningful word pairs.  \n",
        "Typical values: 5â€“50 (implementation-dependent).\n",
        "It ensures pairs like â€œin theâ€ or â€œof aâ€ are **not** merged.\n",
        "\n",
        "A pair is merged into a phrase if its score exceeds a chosen threshold.\n",
        "\n",
        "Example: \"new\" \"york\" $â†’$ \"new_york\"\n",
        "\n",
        "**b. Multi-pass Construction**\n",
        "\n",
        "Phrase detection is repeated:\n",
        "\n",
        "- Pass 1: detect bigrams $â†’$ \"new_york\"\n",
        "- Pass 2: detect larger phrases $â†’$ \"new_york_times\"\n",
        "\n",
        "Each pass uses the same scoring formula but applied to the updated corpus.\n",
        "\n",
        "**c. Training Phrase Vectors**\n",
        "\n",
        "Once merged, phrase tokens appear directly in the corpus and are treated as standard vocabulary items.\n",
        "Skip-gram/CBOW then learns embeddings for tokens like \"new_york\" and \"new_york_times\" the same way it learns single-word vectors."
      ],
      "metadata": {
        "id": "ZJB3sCrz9943"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Letâ€™s reflect on the word2vec design by taking the skip-gram model as an example. What is the relationship between the dot product of two word vectors in the skip-gram model and the cosine similarity? For a pair of words with similar semantics, why may the cosine similarity of their word vectors (trained by the skip-gram model) be high?"
      ],
      "metadata": {
        "id": "9rQzium4vkvo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the Skip-gram model, the probability of predicting a context word \\(w_o\\) from a center word \\(w_c\\) depends on the dot product:\n",
        "\n",
        "$$\n",
        "p(w_o \\mid w_c) \\sim \\exp(u_o^T v_c)\n",
        "$$\n",
        "\n",
        "Thus, **the dot product directly measures how strongly two words are related** in the model.\n",
        "\n",
        "**Relationship to Cosine Similarity**\n",
        "\n",
        "For any vectors $a$ and $b$:\n",
        "\n",
        "$$\n",
        "\\cos(a,b) = \\frac{a^T b}{\\|a\\|\\,\\|b\\|}\n",
        "$$\n",
        "\n",
        "This shows:\n",
        "- The dot product is the **numerator** of cosine similarity.\n",
        "- Cosine similarity simply normalizes the dot product so that only direction (not vector length) matters.\n",
        "\n",
        "**Why Semantically Similar Words Have High Cosine Similarity**\n",
        "- Words with similar meanings appear in similar contexts: In Skip-gram, such words receive similar gradient updates because they predict many of the same context words.\n",
        "- Their vectors become aligned: The model pushes both words to have large dot products with similar sets of context vectors, forcing their embedding directions to be close.\n",
        "- Cosine similarity measures directional closeness: Since similar words end up pointing in nearly the same direction, their cosine similarity becomes high.\n"
      ],
      "metadata": {
        "id": "OOI7fD1qSJ02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisite Mathematical Concepts\n",
        "\n",
        "Before diving into the Word2Vec models, we need to understand three fundamental mathematical concepts that form the backbone of these algorithms:\n",
        "\n",
        "1. **Conditional Probability** - Understanding how the probability of one event changes given information about another event\n",
        "2. **Maximum Likelihood Estimation (MLE)** - A method for finding the best parameters that explain our observed data\n",
        "3. **Softmax Function** - A way to convert raw scores into probabilities\n",
        "\n",
        "Let's explore each concept in detail with step-by-step explanations and examples."
      ],
      "metadata": {
        "id": "ORJYScWcxCyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.b Conditional Probability\n",
        "\n",
        "### What is Conditional Probability?\n",
        "\n",
        "**Conditional probability** answers the question: *\"What is the probability of event A happening, given that we already know event B has happened?\"*\n",
        "\n",
        "In everyday language, we use conditional probability all the time:\n",
        "- \"What's the chance of rain **given that** the sky is cloudy?\"\n",
        "- \"What's the probability a student passes the exam **given that** they studied for 10 hours?\"\n",
        "\n",
        "In Word2Vec, we ask similar questions:\n",
        "- \"What's the probability of seeing the word 'cat' **given that** the center word is 'pet'?\"\n",
        "- \"What's the probability of the center word being 'coffee' **given that** the surrounding words are 'I', 'drink', 'every', 'morning'?\"\n",
        "\n",
        "### The Formal Definition\n",
        "\n",
        "The conditional probability of event $A$ given event $B$ is written as $P(A \\mid B)$ and is defined as:\n",
        "\n",
        "$$P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
        "\n",
        "Where:\n",
        "- $P(A \\mid B)$ is read as \"the probability of A given B\"\n",
        "- $P(A \\cap B)$ is the probability that **both** A and B happen (the intersection)\n",
        "- $P(B)$ is the probability that B happens\n",
        "- **Important**: This formula only makes sense when $P(B) > 0$ (we can't condition on an impossible event)"
      ],
      "metadata": {
        "id": "7AxsQWCtxHUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intuitive Understanding\n",
        "\n",
        "Think of conditional probability as **narrowing down your sample space**.\n",
        "\n",
        "Imagine you have a bag with 10 balls:\n",
        "- 4 red balls\n",
        "- 6 blue balls\n",
        "- Among the red balls: 3 are large, 1 is small\n",
        "- Among the blue balls: 2 are large, 4 are small\n",
        "\n",
        "**Question 1**: What's the probability of picking a large ball?\n",
        "- Total large balls = 3 + 2 = 5\n",
        "- Total balls = 10\n",
        "- $P(\\text{Large}) = \\frac{5}{10} = 0.5$\n",
        "\n",
        "**Question 2**: What's the probability of picking a large ball, **given that** we already know it's red?\n",
        "- Now we only consider the red balls (our sample space is reduced!)\n",
        "- Red balls = 4\n",
        "- Large red balls = 3\n",
        "- $P(\\text{Large} \\mid \\text{Red}) = \\frac{3}{4} = 0.75$\n",
        "\n",
        "Notice how knowing the ball is red **changed** the probability from 0.5 to 0.75!"
      ],
      "metadata": {
        "id": "NlwpItipxPv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step-by-Step Verification Using the Formula\n",
        "\n",
        "Let's verify our answer using the formal formula $P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}$\n",
        "\n",
        "**Given**:\n",
        "- Event $A$ = picking a large ball\n",
        "- Event $B$ = picking a red ball\n",
        "\n",
        "**Step 1**: Calculate $P(B)$ - the probability of picking a red ball\n",
        "$$P(\\text{Red}) = \\frac{\\text{Number of red balls}}{\\text{Total balls}} = \\frac{4}{10} = 0.4$$\n",
        "\n",
        "**Step 2**: Calculate $P(A \\cap B)$ - the probability of picking a ball that is BOTH large AND red\n",
        "$$P(\\text{Large} \\cap \\text{Red}) = \\frac{\\text{Number of large red balls}}{\\text{Total balls}} = \\frac{3}{10} = 0.3$$\n",
        "\n",
        "**Step 3**: Apply the conditional probability formula\n",
        "$$P(\\text{Large} \\mid \\text{Red}) = \\frac{P(\\text{Large} \\cap \\text{Red})}{P(\\text{Red})} = \\frac{0.3}{0.4} = \\frac{3}{4} = 0.75$$\n",
        "\n",
        "This matches our intuitive answer of $0.75$."
      ],
      "metadata": {
        "id": "eyqEzkR6xU3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Chain Rule of Probability\n",
        "\n",
        "From the conditional probability formula, we can derive an extremely useful result called the **Chain Rule**:\n",
        "\n",
        "Starting from:\n",
        "$$P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
        "\n",
        "We can rearrange by multiplying both sides by $P(B)$:\n",
        "$$P(A \\cap B) = P(A \\mid B) \\times P(B)$$\n",
        "\n",
        "This tells us: **The probability of A AND B happening equals the probability of B happening times the probability of A given B**.\n",
        "\n",
        "**Example**: What's the probability of drawing two aces in a row from a deck of cards (without replacement)?\n",
        "\n",
        "Let:\n",
        "- $A$ = second card is an ace\n",
        "- $B$ = first card is an ace\n",
        "\n",
        "**Step 1**: $P(B) = P(\\text{First card is ace}) = \\frac{4}{52} = \\frac{1}{13}$\n",
        "\n",
        "**Step 2**: $P(A \\mid B) = P(\\text{Second card is ace} \\mid \\text{First card was ace})$\n",
        "- After drawing one ace, there are 3 aces left and 51 cards total\n",
        "- $P(A \\mid B) = \\frac{3}{51} = \\frac{1}{17}$\n",
        "\n",
        "**Step 3**: Apply chain rule\n",
        "$$P(A \\cap B) = P(A \\mid B) \\times P(B) = \\frac{1}{17} \\times \\frac{1}{13} = \\frac{1}{221} \\approx 0.0045$$\n",
        "\n",
        "### Extension to Multiple Events\n",
        "\n",
        "The chain rule extends to multiple events:\n",
        "$$P(A, B, C) = P(A) \\times P(B \\mid A) \\times P(C \\mid A, B)$$\n",
        "\n",
        "In general, for events $X_1, X_2, \\ldots, X_n$:\n",
        "$$P(X_1, X_2, \\ldots, X_n) = P(X_1) \\times P(X_2 \\mid X_1) \\times P(X_3 \\mid X_1, X_2) \\times \\cdots \\times P(X_n \\mid X_1, \\ldots, X_{n-1})$$\n",
        "\n",
        "This is crucial for modeling sequences of words in NLP!"
      ],
      "metadata": {
        "id": "PJyHkifhxZqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connection to Word2Vec\n",
        "\n",
        "In Word2Vec, conditional probability is the core concept that drives how word embeddings are learned.\n",
        "\n",
        "**Skip-Gram Model** asks: Given a center word, what is the probability of each context word?\n",
        "$$P(w_{\\text{context}} \\mid w_{\\text{center}})$$\n",
        "\n",
        "For example, given the sentence \"The **cat** sat on the mat\" with \"cat\" as the center word and window size 2:\n",
        "- $P(\\text{\"The\"} \\mid \\text{\"cat\"}) = ?$\n",
        "- $P(\\text{\"sat\"} \\mid \\text{\"cat\"}) = ?$\n",
        "- $P(\\text{\"on\"} \\mid \\text{\"cat\"}) = ?$\n",
        "\n",
        "**CBOW Model** asks: Given the context words, what is the probability of the center word?\n",
        "$$P(w_{\\text{center}} \\mid w_{\\text{context}_1}, w_{\\text{context}_2}, \\ldots)$$\n",
        "\n",
        "For example: $P(\\text{\"cat\"} \\mid \\text{\"The\"}, \\text{\"sat\"}, \\text{\"on\"}) = ?$\n",
        "\n",
        "**Key Insight**: Word2Vec learns word vectors such that words appearing in similar contexts have similar vectors. The conditional probability framework allows us to:\n",
        "1. Formulate this as a prediction problem\n",
        "2. Use maximum likelihood (next section) to find the best word vectors\n",
        "3. Use softmax (later section) to convert similarity scores into probabilities\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ivwO7jgdxelx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.c Maximum Likelihood Estimation (MLE)\n",
        "\n",
        "### What is Maximum Likelihood Estimation?\n",
        "\n",
        "**Maximum Likelihood Estimation (MLE)** is a method for finding the best parameters of a statistical model. The idea is simple:\n",
        "\n",
        "> *\"Find the parameter values that make the observed data most probable.\"*\n",
        "\n",
        "In other words, among all possible parameter values, we choose the ones that would most likely produce the data we actually observed.\n",
        "\n",
        "### Likelihood vs Probability: Understanding the Difference\n",
        "\n",
        "This distinction is subtle but crucial:\n",
        "\n",
        "**Probability**: Given fixed parameters, what's the chance of observing certain data?\n",
        "- \"If a coin has 50% heads probability, what's the chance of getting 7 heads in 10 flips?\"\n",
        "- We fix $\\theta = 0.5$ and ask about $P(\\text{data})$\n",
        "\n",
        "**Likelihood**: Given observed data, how plausible are different parameter values?\n",
        "- \"I observed 7 heads in 10 flips. Is the coin's heads probability more likely 0.5 or 0.7?\"\n",
        "- We fix the data and ask about different values of $\\theta$\n",
        "\n",
        "**Mathematical notation**:\n",
        "- Probability: $P(\\text{Data} \\mid \\theta)$ - probability of data given fixed parameter $\\theta$\n",
        "- Likelihood: $\\mathcal{L}(\\theta \\mid \\text{Data})$ - likelihood of parameter $\\theta$ given fixed data\n",
        "\n",
        "Numerically, they are the same value! But conceptually:\n",
        "- In probability, $\\theta$ is fixed and Data varies\n",
        "- In likelihood, Data is fixed and $\\theta$ varies"
      ],
      "metadata": {
        "id": "gOU_ETKexinp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The MLE Formula\n",
        "\n",
        "The Maximum Likelihood Estimate is the parameter value $\\hat{\\theta}$ that maximizes the likelihood:\n",
        "\n",
        "$$\\hat{\\theta}_{MLE} = \\underset{\\theta}{\\arg\\max} \\; \\mathcal{L}(\\theta \\mid \\text{Data}) = \\underset{\\theta}{\\arg\\max} \\; P(\\text{Data} \\mid \\theta)$$\n",
        "\n",
        "Where:\n",
        "- $\\hat{\\theta}_{MLE}$ is the MLE estimate (the \"best\" parameter value)\n",
        "- $\\underset{\\theta}{\\arg\\max}$ means \"find the value of $\\theta$ that maximizes...\"\n",
        "- $\\mathcal{L}(\\theta \\mid \\text{Data})$ is the likelihood function\n",
        "\n",
        "### Step-by-Step Example: The Coin Flip Problem\n",
        "\n",
        "**Problem**: You flip a coin 10 times and observe 7 heads and 3 tails. What is the most likely value for the coin's probability of heads?\n",
        "\n",
        "**Setup**:\n",
        "- Let $\\theta$ = probability of heads (what we want to find)\n",
        "- Observed data: 7 heads (H) and 3 tails (T) in 10 flips\n",
        "- Each flip is independent\n",
        "\n",
        "**Step 1: Write the likelihood function**\n",
        "\n",
        "For a single coin flip:\n",
        "- $P(\\text{Heads} \\mid \\theta) = \\theta$\n",
        "- $P(\\text{Tails} \\mid \\theta) = 1 - \\theta$\n",
        "\n",
        "Since all flips are independent, we multiply the probabilities:\n",
        "$$\\mathcal{L}(\\theta) = P(\\text{7 heads, 3 tails} \\mid \\theta) = \\theta^7 \\times (1-\\theta)^3$$\n",
        "\n",
        "**Step 2: Try some values to build intuition**\n",
        "\n",
        "Let's calculate the likelihood for different values of $\\theta$:\n",
        "\n",
        "| $\\theta$ | $\\theta^7$ | $(1-\\theta)^3$ | $\\mathcal{L}(\\theta) = \\theta^7 \\times (1-\\theta)^3$ |\n",
        "|----------|------------|----------------|-----------------------------------------------------|\n",
        "| 0.5 | 0.0078 | 0.125 | 0.000977 |\n",
        "| 0.6 | 0.0280 | 0.064 | 0.001792 |\n",
        "| 0.7 | 0.0824 | 0.027 | 0.002224 |\n",
        "| 0.8 | 0.2097 | 0.008 | 0.001678 |\n",
        "\n",
        "Notice that $\\theta = 0.7$ gives the highest likelihood! This matches our intuition: 7 heads out of 10 suggests the probability should be around 70%."
      ],
      "metadata": {
        "id": "TzIof8TkxoEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Find the exact maximum using calculus**\n",
        "\n",
        "To find the exact value of $\\theta$ that maximizes the likelihood, we take the derivative and set it to zero.\n",
        "\n",
        "Our likelihood function is:\n",
        "$$\\mathcal{L}(\\theta) = \\theta^7 \\times (1-\\theta)^3$$\n",
        "\n",
        "**Step 3a**: Apply the product rule to find $\\frac{d\\mathcal{L}}{d\\theta}$\n",
        "\n",
        "Product rule: $\\frac{d}{dx}[f(x) \\cdot g(x)] = f'(x) \\cdot g(x) + f(x) \\cdot g'(x)$\n",
        "\n",
        "Let $f(\\theta) = \\theta^7$ and $g(\\theta) = (1-\\theta)^3$\n",
        "\n",
        "Then:\n",
        "- $f'(\\theta) = 7\\theta^6$ (power rule)\n",
        "- $g'(\\theta) = 3(1-\\theta)^2 \\times (-1) = -3(1-\\theta)^2$ (chain rule)\n",
        "\n",
        "Therefore:\n",
        "$$\\frac{d\\mathcal{L}}{d\\theta} = 7\\theta^6 \\cdot (1-\\theta)^3 + \\theta^7 \\cdot (-3)(1-\\theta)^2$$\n",
        "\n",
        "**Step 3b**: Simplify\n",
        "\n",
        "Factor out common terms $\\theta^6(1-\\theta)^2$:\n",
        "$$\\frac{d\\mathcal{L}}{d\\theta} = \\theta^6(1-\\theta)^2 \\left[ 7(1-\\theta) - 3\\theta \\right]$$\n",
        "\n",
        "Simplify the bracket:\n",
        "$$= \\theta^6(1-\\theta)^2 \\left[ 7 - 7\\theta - 3\\theta \\right]$$\n",
        "$$= \\theta^6(1-\\theta)^2 \\left[ 7 - 10\\theta \\right]$$\n",
        "\n",
        "**Step 3c**: Set derivative to zero and solve\n",
        "\n",
        "$$\\theta^6(1-\\theta)^2(7 - 10\\theta) = 0$$\n",
        "\n",
        "This equals zero when:\n",
        "- $\\theta = 0$ (trivial solution - coin never shows heads)\n",
        "- $\\theta = 1$ (trivial solution - coin always shows heads)\n",
        "- $7 - 10\\theta = 0 \\Rightarrow \\theta = 0.7$ (**this is our MLE!**)\n",
        "\n",
        "**Conclusion**: $\\hat{\\theta}_{MLE} = 0.7 = \\frac{7}{10} = \\frac{\\text{number of heads}}{\\text{total flips}}$\n",
        "\n",
        "This is a beautiful result! The MLE for a coin's bias is simply the observed proportion of heads."
      ],
      "metadata": {
        "id": "CXC_WVM9xtoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Use Log-Likelihood?\n",
        "\n",
        "In practice, we almost always work with the **log-likelihood** instead of the likelihood directly:\n",
        "\n",
        "$$\\ell(\\theta) = \\log \\mathcal{L}(\\theta)$$\n",
        "\n",
        "**Why take the logarithm?**\n",
        "\n",
        "**Reason 1: Numerical Stability**\n",
        "\n",
        "When we have many data points, the likelihood becomes a product of many small probabilities:\n",
        "$$\\mathcal{L}(\\theta) = P(x_1|\\theta) \\times P(x_2|\\theta) \\times \\cdots \\times P(x_n|\\theta)$$\n",
        "\n",
        "For example, with 1000 data points each with probability 0.01:\n",
        "$$\\mathcal{L}(\\theta) = 0.01^{1000} = 10^{-2000}$$\n",
        "\n",
        "This number is so small that computers cannot represent it (underflow)!\n",
        "\n",
        "**Reason 2: Products Become Sums**\n",
        "\n",
        "The logarithm transforms products into sums:\n",
        "$$\\log(a \\times b) = \\log(a) + \\log(b)$$\n",
        "\n",
        "So:\n",
        "$$\\ell(\\theta) = \\log \\mathcal{L}(\\theta) = \\log P(x_1|\\theta) + \\log P(x_2|\\theta) + \\cdots + \\log P(x_n|\\theta)$$\n",
        "\n",
        "$$= \\sum_{i=1}^{n} \\log P(x_i|\\theta)$$\n",
        "\n",
        "Sums are much easier to compute and differentiate than products!\n",
        "\n",
        "**Reason 3: Same Maximum**\n",
        "\n",
        "Since $\\log$ is a monotonically increasing function:\n",
        "- If $a > b$, then $\\log(a) > \\log(b)$\n",
        "\n",
        "Therefore, maximizing $\\mathcal{L}(\\theta)$ is equivalent to maximizing $\\log \\mathcal{L}(\\theta)$:\n",
        "$$\\underset{\\theta}{\\arg\\max} \\; \\mathcal{L}(\\theta) = \\underset{\\theta}{\\arg\\max} \\; \\log \\mathcal{L}(\\theta)$$"
      ],
      "metadata": {
        "id": "Yge5Js_dxzLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coin Flip Example with Log-Likelihood\n",
        "\n",
        "Let's redo our coin flip example using log-likelihood.\n",
        "\n",
        "**Original likelihood**:\n",
        "$$\\mathcal{L}(\\theta) = \\theta^7 \\times (1-\\theta)^3$$\n",
        "\n",
        "**Step 1: Take the logarithm**\n",
        "$$\\ell(\\theta) = \\log(\\theta^7 \\times (1-\\theta)^3)$$\n",
        "\n",
        "Using $\\log(a \\times b) = \\log(a) + \\log(b)$:\n",
        "$$\\ell(\\theta) = \\log(\\theta^7) + \\log((1-\\theta)^3)$$\n",
        "\n",
        "Using $\\log(a^n) = n \\times \\log(a)$:\n",
        "$$\\ell(\\theta) = 7\\log(\\theta) + 3\\log(1-\\theta)$$\n",
        "\n",
        "**Step 2: Find the derivative**\n",
        "$$\\frac{d\\ell}{d\\theta} = 7 \\times \\frac{1}{\\theta} + 3 \\times \\frac{-1}{1-\\theta}$$\n",
        "$$= \\frac{7}{\\theta} - \\frac{3}{1-\\theta}$$\n",
        "\n",
        "**Step 3: Set to zero and solve**\n",
        "$$\\frac{7}{\\theta} - \\frac{3}{1-\\theta} = 0$$\n",
        "\n",
        "Multiply both sides by $\\theta(1-\\theta)$:\n",
        "$$7(1-\\theta) - 3\\theta = 0$$\n",
        "$$7 - 7\\theta - 3\\theta = 0$$\n",
        "$$7 = 10\\theta$$\n",
        "$$\\theta = 0.7$$\n",
        "\n",
        "**Same answer**, but the calculation was simpler! No product rule needed."
      ],
      "metadata": {
        "id": "Tues9NvNx1j3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From MLE to Minimizing Loss: The Word2Vec Connection\n",
        "\n",
        "In Word2Vec, we use MLE to find the best word vectors. Here's how it works:\n",
        "\n",
        "**The Setup**:\n",
        "- We have a text corpus with words $w_1, w_2, \\ldots, w_T$\n",
        "- We want to find word vectors (parameters $\\theta$) that maximize the probability of observing these word sequences\n",
        "\n",
        "**For Skip-Gram**: Given center word $w_c$, predict context words $w_o$\n",
        "\n",
        "The likelihood of the entire corpus is:\n",
        "$$\\mathcal{L}(\\theta) = \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} P(w^{(t+j)} \\mid w^{(t)}; \\theta)$$\n",
        "\n",
        "**Taking the log**:\n",
        "$$\\ell(\\theta) = \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P(w^{(t+j)} \\mid w^{(t)}; \\theta)$$\n",
        "\n",
        "**The Key Transformation: Maximization â†’ Minimization**\n",
        "\n",
        "In machine learning, we typically **minimize** a loss function rather than maximize likelihood. The conversion is simple:\n",
        "\n",
        "$$\\text{Maximize } \\ell(\\theta) \\quad \\Longleftrightarrow \\quad \\text{Minimize } -\\ell(\\theta)$$\n",
        "\n",
        "So the **loss function** for Skip-Gram is:\n",
        "$$\\mathcal{L}_{loss} = -\\ell(\\theta) = -\\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P(w^{(t+j)} \\mid w^{(t)}; \\theta)$$\n",
        "\n",
        "This is called **Negative Log-Likelihood (NLL)** loss!\n",
        "\n",
        "**Why minimize negative log-likelihood?**\n",
        "1. Convention: ML frameworks are designed to minimize, not maximize\n",
        "2. Positive values: Loss is always non-negative (since $\\log P \\leq 0$ for $P \\leq 1$)\n",
        "3. Interpretation: Lower loss = higher likelihood = better model\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9KRvytFfx9A8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.d The Softmax Function\n",
        "\n",
        "### What is Softmax?\n",
        "\n",
        "The **softmax function** converts a vector of arbitrary real numbers into a **probability distribution**. It takes a vector of \"scores\" (which can be any real numbers, positive or negative) and outputs a vector of probabilities that:\n",
        "1. Are all positive (between 0 and 1)\n",
        "2. Sum to exactly 1\n",
        "\n",
        "This makes softmax perfect for multi-class classification where we need to predict the probability of each class.\n",
        "\n",
        "### The Softmax Formula\n",
        "\n",
        "Given a vector of scores $\\mathbf{z} = (z_1, z_2, \\ldots, z_K)$ where $K$ is the number of classes, the softmax function outputs:\n",
        "\n",
        "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
        "\n",
        "Or for the entire vector:\n",
        "$$\\text{softmax}(\\mathbf{z}) = \\left( \\frac{e^{z_1}}{\\sum_{j=1}^{K} e^{z_j}}, \\frac{e^{z_2}}{\\sum_{j=1}^{K} e^{z_j}}, \\ldots, \\frac{e^{z_K}}{\\sum_{j=1}^{K} e^{z_j}} \\right)$$\n",
        "\n",
        "Where:\n",
        "- $e \\approx 2.71828$ is Euler's number (base of natural logarithm)\n",
        "- $e^{z_i}$ is the exponential of $z_i$\n",
        "- The denominator $\\sum_{j=1}^{K} e^{z_j}$ is a **normalization constant** that ensures the outputs sum to 1"
      ],
      "metadata": {
        "id": "1JN1NTQbyH9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step-by-Step Numerical Example\n",
        "\n",
        "Let's compute softmax for the vector $\\mathbf{z} = (2, 1, 0.5)$\n",
        "\n",
        "**Step 1: Compute the exponential of each element**\n",
        "\n",
        "| $i$ | $z_i$ | $e^{z_i}$ | Calculation |\n",
        "|-----|-------|-----------|-------------|\n",
        "| 1 | 2 | $e^2 = 7.389$ | $2.71828^2 = 7.389$ |\n",
        "| 2 | 1 | $e^1 = 2.718$ | $2.71828^1 = 2.718$ |\n",
        "| 3 | 0.5 | $e^{0.5} = 1.649$ | $2.71828^{0.5} = 1.649$ |\n",
        "\n",
        "**Step 2: Calculate the sum of all exponentials (denominator)**\n",
        "\n",
        "$$\\sum_{j=1}^{3} e^{z_j} = e^2 + e^1 + e^{0.5} = 7.389 + 2.718 + 1.649 = 11.756$$\n",
        "\n",
        "**Step 3: Divide each exponential by the sum**\n",
        "\n",
        "| $i$ | $e^{z_i}$ | $\\text{softmax}(z_i) = \\frac{e^{z_i}}{11.756}$ | Probability |\n",
        "|-----|-----------|------------------------------------------------|-------------|\n",
        "| 1 | 7.389 | $\\frac{7.389}{11.756}$ | $0.628$ (62.8%) |\n",
        "| 2 | 2.718 | $\\frac{2.718}{11.756}$ | $0.231$ (23.1%) |\n",
        "| 3 | 1.649 | $\\frac{1.649}{11.756}$ | $0.140$ (14.0%) |\n",
        "\n",
        "**Step 4: Verify probabilities sum to 1**\n",
        "\n",
        "$$0.628 + 0.231 + 0.140 = 0.999 \\approx 1.0 \\; \\checkmark$$\n",
        "\n",
        "(The small error is due to rounding)\n",
        "\n",
        "**Interpretation**: The input with the highest score (2) gets the highest probability (62.8%), but the other classes still get non-zero probabilities."
      ],
      "metadata": {
        "id": "1-ndppPnyKbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Use the Exponential Function?\n",
        "\n",
        "You might wonder: why $e^{z_i}$ instead of just $z_i$? There are several important reasons:\n",
        "\n",
        "**1. Always Positive**\n",
        "\n",
        "The exponential function $e^x$ is always positive for any real number $x$:\n",
        "- $e^{-100} = 3.7 \\times 10^{-44}$ (very small, but positive!)\n",
        "- $e^{0} = 1$\n",
        "- $e^{100} = 2.7 \\times 10^{43}$ (very large)\n",
        "\n",
        "This ensures all probabilities are positive, which is required!\n",
        "\n",
        "**2. Amplifies Differences**\n",
        "\n",
        "The exponential function amplifies differences between scores:\n",
        "\n",
        "| $z_1$ | $z_2$ | Difference | $e^{z_1}$ | $e^{z_2}$ | Ratio $\\frac{e^{z_1}}{e^{z_2}}$ |\n",
        "|-------|-------|------------|-----------|-----------|-------------------------------|\n",
        "| 3 | 2 | 1 | 20.09 | 7.39 | 2.72 |\n",
        "| 5 | 2 | 3 | 148.4 | 7.39 | 20.09 |\n",
        "\n",
        "A score difference of 1 leads to a probability ratio of about 2.72 ($\\approx e$).\n",
        "\n",
        "**3. Differentiable**\n",
        "\n",
        "The exponential function has a nice derivative: $\\frac{d}{dx}e^x = e^x$\n",
        "\n",
        "This makes it easy to compute gradients for training neural networks with gradient descent.\n",
        "\n",
        "**4. Preserves Ranking**\n",
        "\n",
        "If $z_1 > z_2$, then $e^{z_1} > e^{z_2}$, so $\\text{softmax}(z_1) > \\text{softmax}(z_2)$\n",
        "\n",
        "The relative ordering is preserved."
      ],
      "metadata": {
        "id": "j_pp9ukJyN59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connection to Word2Vec\n",
        "\n",
        "In Word2Vec, the softmax function is crucial for converting similarity scores between word vectors into probabilities.\n",
        "\n",
        "**Skip-Gram Model**: Given a center word $w_c$ with vector $\\mathbf{v}_c$, we want to compute the probability of each word $w_o$ being a context word.\n",
        "\n",
        "**Step 1**: Compute the \"score\" for each word using dot product\n",
        "\n",
        "The dot product $\\mathbf{u}_o^\\top \\mathbf{v}_c$ measures how similar the context word vector $\\mathbf{u}_o$ is to the center word vector $\\mathbf{v}_c$:\n",
        "- High dot product â†’ words are similar â†’ high score\n",
        "- Low dot product â†’ words are different â†’ low score\n",
        "\n",
        "**Step 2**: Convert scores to probabilities using softmax\n",
        "\n",
        "$$P(w_o \\mid w_c) = \\text{softmax}(\\mathbf{u}_o^\\top \\mathbf{v}_c) = \\frac{\\exp(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)}$$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{u}_o$ is the \"context\" vector for word $w_o$\n",
        "- $\\mathbf{v}_c$ is the \"center\" vector for word $w_c$\n",
        "- $\\mathcal{V}$ is the entire vocabulary\n",
        "- The denominator sums over ALL words in the vocabulary\n",
        "\n",
        "**Example Intuition**:\n",
        "\n",
        "Consider the center word \"coffee\" and three potential context words:\n",
        "\n",
        "| Context Word | Dot Product Score | After Softmax |\n",
        "|--------------|-------------------|---------------|\n",
        "| \"drink\" | 5.0 (high similarity) | 0.72 |\n",
        "| \"morning\" | 3.5 (moderate similarity) | 0.24 |\n",
        "| \"airplane\" | 0.5 (low similarity) | 0.04 |\n",
        "\n",
        "The softmax converts these raw similarity scores into a valid probability distribution!\n",
        "\n",
        "**The Computational Challenge**:\n",
        "\n",
        "Notice that the denominator $\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)$ sums over the **entire vocabulary**.\n",
        "\n",
        "If the vocabulary has 3 million words, this means:\n",
        "- 3 million dot products\n",
        "- 3 million exponentials\n",
        "- 3 million additions\n",
        "\n",
        "**For every single training example!** This is why approximate methods like Negative Sampling (covered earlier in this notebook) are used in practice.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "FhOOXLP-yQRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do we need to use approximate training instead of the full softmax?"
      ],
      "metadata": {
        "id": "kH9vArdkfpm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fully Softmax\n",
        "In general, according to (15.1.4)\n",
        "the log conditional probability\n",
        "involving any pair of the center word $w_c$ and\n",
        "the context word $w_o$ is\n",
        "\n",
        "$$\\log P(w_o \\mid w_c) =\\mathbf{u}_o^\\top \\mathbf{v}_c - \\log\\left(\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)\\right).$$\n",
        "\n",
        "where $\\mathcal{|V|}$ = vocabulary size so complexity for full softmax cost per training pair\n",
        "\n",
        "$$O(\\mathcal{|V|})$$\n",
        "\n",
        "\n",
        "For a typical Word2Vec with a vocabulary size of $|V| = 3{,}000{,}000$:\n",
        "\n",
        "- $\\approx 3{,}000{,}000$ dot products per update  \n",
        "- $\\approx 3{,}000{,}000$ gradient updates\n",
        "\n",
        "## Negative Sampling\n",
        "\n",
        "Replace the full softmax probability with a binary logistic classifier (Nagetive Sampling):\n",
        "\n",
        "- For each real pair (context, target):\n",
        "\n",
        "  - Predict $D = 1$\n",
        "\n",
        "- For k noise words sampled from a unigram distribution:\n",
        "\n",
        "  - Predict $D = 0$\n",
        "\n",
        "**Cost per step:**\n",
        "\n",
        "$$O(\\mathcal{k})$$\n",
        "\n",
        "where typically $k=5$ to $10$ $15$.\n",
        "\n",
        "So instead of 3M updates, you only do ~10 updates. Massive speedup $\\approx 500,000$x faster than full softmax.\n",
        "\n",
        "## Hierarchical Softmax\n",
        "\n",
        "Use a Huffman tree to reduce softmax from:\n",
        "\n",
        "$$O(\\mathcal{|V|})$$\n",
        "\n",
        "to:\n",
        "\n",
        "$$O(\\log_2{|V|})$$\n",
        "\n",
        "For a vocabulary size of 3M words:\n",
        "\n",
        "$$O(\\log_2{|3,000,000|}) \\approx 22$$\n",
        "\n",
        "So each update uses $\\log$ ~20 nodes instead of millions."
      ],
      "metadata": {
        "id": "M3CgJIpGARdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Negative Sampling in Skip-Gram Model (SG)\n",
        "\n",
        "Negative sampling transform the multi-classification into binary classification.\n",
        "\n",
        "Given:\n",
        "- The center word $w_c$\n",
        "- The context word $w_o$\n",
        "\n",
        "The probability model will be:\n",
        "\n",
        "$$P(D=1\\mid w_c, w_o) = \\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c)$$\n",
        "\n",
        "Based on (15.1.5), given:\n",
        "- The text sequence of length $T$\n",
        "- The word at time step $t$ is $w^{(t)}$\n",
        "- The context window size be $m$\n",
        "\n",
        "The joint probability will be:\n",
        "\n",
        "$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(D=1\\mid w^{(t)}, w^{(t+j)})$$\n",
        "\n",
        "The formula only considers those events that involve positive examples ($D = 1$). The joint probability is maximized to $1$ only if $v_c^\\top v_w \\to +\\infty$. In other words, all the word vectors are equal to infinity. We are expecting that adding negative examples ($D = 0$) will make more sense.\n",
        "\n",
        "Given:\n",
        "- $S$ is the event that a context word $w_o$ comes from the context window of a center word $w_c$\n",
        "- With predefined distribution $P(w)$ sample $K$ noise words, $N_k$ is the event that a noise word $w_k$ ($k = 1, ..., K$)\n",
        "\n",
        "So these events involving both the positive example and negative examples are $\\{S, N_1, ..., N_K \\}$.\n",
        "\n",
        "Negative sampling rewrites the conditional probability:\n",
        "\n",
        "$$P(w^{(t+j)} \\mid w^{(t)})$$\n",
        "$$= P_S \\prod_K P_k $$\n",
        "$$= P(D=1\\mid w^{(t)}, w^{(t+j)}) \\prod_{k=1,\\,w_k \\sim P(w)}^K P(D=0\\mid w^{(t)}, w_k) $$\n",
        "\n",
        "Given:\n",
        "- $i_t$ is index of a word $w^{(t)}$ at time step $t$\n",
        "- $h_k$ is index of a noise word $w_k$\n",
        "\n",
        "The logarithmic loss\n",
        "\n",
        "$$-\\log P(w^{(t+j)} \\mid w^{(t)})$$\n",
        "$$= -\\log P(D=1\\mid w^{(t)}, w^{(t+j)}) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log P(D=0\\mid w^{(t)}, w_k)$$\n",
        "\n",
        "because of classification binary so $P(D=0\\mid w^{(t)}, w_k) = 1 - P(D=1\\mid w^{(t)}, w_k)$, we have:\n",
        "\n",
        "$$= -\\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\left(1-\\sigma\\left(\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)\\right)$$\n",
        "\n",
        "with $\\sigma(x) + \\sigma(-x) = 1$, We can infer:\n",
        "\n",
        "$$= -\\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\sigma\\left(-\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KEazcRt8Y8cQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Negative Sampling in Conitnuous Bag-of-Word Model (CBOW)\n",
        "\n",
        "Given:\n",
        "- The center word $w_c$\n",
        "- The context vector $\\bar{\\mathbf{v}}_o = \\left(\\mathbf{v}_{o_1} + \\ldots + \\mathbf{v}_{o_{2m}} \\right)/(2m)$\n",
        "\n",
        "The probability model will be:\n",
        "\n",
        "$$P(D=1\\mid w_c, w_{o_1},..., w_{o_{2m}}) = \\sigma(\\mathbf{u}_o^\\top \\bar{\\mathbf{v}}_o)$$\n",
        "\n",
        "Based on (15.1.12), given:\n",
        "- The text sequence of length $T$\n",
        "- The word at time step $t$ is $w^{(t)}$\n",
        "- The context window size be $m$\n",
        "\n",
        "The joint probability will be:\n",
        "\n",
        "$$ \\prod_{t=1}^{T}  P(D=1 \\mid w^{(t)},  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)})$$\n",
        "\n",
        "Given:\n",
        "- $S$ is the event that a context word $w_o$ comes from the context window of a center word $w_c$\n",
        "- With predefined distribution $P(w)$ sample $K$ noise words, $N_k$ is the event that a noise word $w_k$ ($k = 1, ..., K$)\n",
        "\n",
        "Add negative examples\n",
        "\n",
        "$$P(w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}) \\mid w^{(t)})$$\n",
        "$$= P_S \\prod_K P_k $$\n",
        "$$= P(D=1\\mid w^{(t)},  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}) \\prod_{k=1,\\,w_k \\sim P(w)}^K P(D=0\\mid w_k, w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}) $$\n",
        "\n",
        "\n",
        "The logarithmic loss:\n",
        "\n",
        "$$-\\log P(w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}) \\mid w^{(t)})$$\n",
        "\n",
        "$$= -\\log P(D=1\\mid w^{(t)},  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log P(D=0\\mid w_k, w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)})$$\n",
        "\n",
        "\n",
        "$$= -\\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\left(1-\\sigma\\left(\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)\\right)$$\n",
        "\n",
        "with $\\sigma(x) + \\sigma(-x) = 1$, We can infer (todo: cleaning):\n",
        "\n",
        "$$= -\\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\bar{\\mathbf{v}}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\sigma\\left(-\\mathbf{u}_{h_k}^\\top \\bar{\\mathbf{v}_{i_t}}\\right)$$\n"
      ],
      "metadata": {
        "id": "jUAyeomkMCuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How can we sample noise words in negative sampling?\n",
        "\n",
        "$$ P_n(w) = \\frac{f(w)^{\\alpha}}{\\sum_{w'} f(w')^{\\alpha}} $$\n",
        "\n",
        "where\n",
        "\n",
        "- $f(w)$: frequency of word $w$ in the corpus\n",
        "- $Î±=0.75$ (chosen empirically by Mikolov et al., 2013)\n"
      ],
      "metadata": {
        "id": "XOg8_cE8n-Wl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verify that (15.2.9) holds\n",
        "\n",
        "$$\\sum_{w \\in \\mathcal{V}} P(w \\mid w_c) = 1.$$\n",
        "\n",
        "Give $P(n)$ is the probability sub tree at $n$:\n",
        "$$P(n) = \\sum_{w \\,under\\, n} P(w \\mid w_c)$$\n",
        "\n",
        "At an internal node $n$, we have two children:\n",
        "- Left child $L$\n",
        "- Right child $R$\n",
        "\n",
        "Let:\n",
        "$$t_n = u_n^\\top\\,v_c$$\n",
        "\n",
        "Then:\n",
        "\n",
        "- Probability of going left:\n",
        "$$p_L = \\sigma(t_n)$$\n",
        "\n",
        "- Probability of going right:\n",
        "$$p_R = \\sigma(-t_n)$$\n",
        "\n",
        "and\n",
        "$$\\sigma(t_n)+\\sigma(-t_n)=1$$\n",
        "\n",
        "Let $Q(n)$ be the probability of reaching node $n$ from the root. So\n",
        "\n",
        "- Probability of all leaves under left child:\n",
        "$$P(L)=Q(n)\\,\\sigma(t_n)$$\n",
        "\n",
        "- Probability of all leaves under right child:\n",
        "$$P(R)=Q(n)\\,\\sigma(-t_n)$$\n",
        "\n",
        "Total probability under node $n$:\n",
        "\n",
        "$$P(n)=P(L)+P(R)$$\n",
        "$$=Q(n)\\,(\\sigma(t_n)+\\sigma(-t_n))=Q(n)$$\n",
        "\n",
        "**The probability mass of a subtree equals the probability of reaching its root**"
      ],
      "metadata": {
        "id": "4-e9Oz94_ykM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Illustration HS](https://d2l.ai/_images/hi-softmax.svg)"
      ],
      "metadata": {
        "id": "YWauRjyq4HAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical Softmax in Skip Gram and Continuous Bag-of-word Model\n",
        "\n",
        "Given:\n",
        "- $L(w)$: number of nodes on the path from the root node to the leaf node representing word $w$ in the binary tree\n",
        "- $n(w, j)$ is the $j^{th}$ node on the path, with context word vector $u_{n(w,j)}$\n",
        "\n",
        "$$P(w_o \\mid w_c) = \\prod_{j=1}^{L(w_o)-1} \\sigma\\left( [\\![  n(w_o, j+1) = \\textrm{leftChild}(n(w_o, j)) ]\\!] \\cdot \\mathbf{u}_{n(w_o, j)}^\\top \\mathbf{v}_c\\right)$$\n",
        "\n",
        "- $\\textrm{leftChild}(n)$ is the left child node of node $n$: if $x$ is true, $[\\![x]\\!] = 1$; otherwise $[\\![x]\\!] = -1$"
      ],
      "metadata": {
        "id": "FGVVpWIdoOAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical Softmax in Continuous Bag-of-word Model\n",
        "\n",
        "Given:\n",
        "- $L(w)$: number of nodes on the path from the root node to the leaf node representing word $w$ in the binary tree\n",
        "- $n(w, j)$ is the $j^{th}$ node on the path, with center word vector $\\theta_{n(w,j)}$\n",
        "- The context vector as the average:\n",
        "\n",
        "$$\\bar{\\mathbf{v}} = \\frac{1}{2m} \\sum_{j=1}^{2m} \\mathbf{v}_{o_j}$$\n",
        "\n",
        "so the conditional probability:\n",
        "\n",
        "\n",
        "$$P(w_o \\mid w_c) = \\prod_{j=1}^{L(w_o)-1} \\sigma\\left( [\\![  n(w_o, j+1) = \\textrm{leftChild}(n(w_o, j)) ]\\!] \\cdot \\theta_{n(w,j)}^\\top \\bar{\\mathbf{v}}\\right)$$\n",
        "\n",
        "- $\\textrm{leftChild}(n)$ is the left child node of node $n$: if $x$ is true, $[\\![x]\\!] = 1$; otherwise $[\\![x]\\!] = -1$"
      ],
      "metadata": {
        "id": "HhlTlXlkyqxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Downgrade setuptools and wheel to compatible versions\n",
        "!pip install setuptools==65.5.0 \"wheel<0.40.0\"\n",
        "\n",
        "# restart your Colab runtime (Runtime â†’ Restart runtime)\n",
        "\n",
        "# 2. Then install d2l\n",
        "!pip install --no-deps d2l==1.0.3"
      ],
      "metadata": {
        "id": "1r0MdeuNEmXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "import heapq\n",
        "import torch.optim as optim\n",
        "import time"
      ],
      "metadata": {
        "id": "GJrWNlrAEvxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "lFlF4lqNajFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.DATA_HUB['ptb'] = (d2l.DATA_URL + 'ptb.zip',\n",
        "                       '319d85e578af0cdc590547f26231e4e31cdf1e42')\n",
        "\n",
        "def read_ptb():\n",
        "    \"\"\"Load the PTB dataset into a list of text lines.\"\"\"\n",
        "    data_dir = d2l.download_extract('ptb')\n",
        "    # Read the training set\n",
        "    with open(os.path.join(data_dir, 'ptb.train.txt')) as f:\n",
        "        raw_text = f.read()\n",
        "    return [line.split() for line in raw_text.split('\\n')]\n",
        "\n",
        "sentences = read_ptb()\n",
        "f'# sentences: {len(sentences)}'"
      ],
      "metadata": {
        "id": "8antLZU3EyFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = d2l.Vocab(sentences, min_freq=10)\n",
        "f'vocab size: {len(vocab)}'"
      ],
      "metadata": {
        "id": "KHw2qGp7E0zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subsampling"
      ],
      "metadata": {
        "id": "txEpW8I4E5gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subsample(sentences, vocab):\n",
        "    \"\"\"Subsample high-frequency words.\"\"\"\n",
        "    # Exclude unknown tokens ('<unk>')\n",
        "    sentences = [[token for token in line if vocab[token] != vocab.unk]\n",
        "                 for line in sentences]\n",
        "    counter = collections.Counter([\n",
        "        token for line in sentences for token in line])\n",
        "    num_tokens = sum(counter.values())\n",
        "\n",
        "    # Return True if `token` is kept during subsampling\n",
        "    def keep(token):\n",
        "        return(random.uniform(0, 1) <\n",
        "               math.sqrt(1e-4 / counter[token] * num_tokens))\n",
        "\n",
        "    return ([[token for token in line if keep(token)] for line in sentences],\n",
        "            counter)\n",
        "\n",
        "subsampled, counter = subsample(sentences, vocab)"
      ],
      "metadata": {
        "id": "gUwJn0c8E36O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.show_list_len_pair_hist(['origin', 'subsampled'], '# tokens per sentence',\n",
        "                            'count', sentences, subsampled);"
      ],
      "metadata": {
        "id": "5XIe6poCasFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_counts(token):\n",
        "    return (f'# of \"{token}\": '\n",
        "            f'before={sum([l.count(token) for l in sentences])}, '\n",
        "            f'after={sum([l.count(token) for l in subsampled])}')\n",
        "\n",
        "compare_counts('the')\n",
        "compare_counts('join')"
      ],
      "metadata": {
        "id": "NPbzgRmMavaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corpus"
      ],
      "metadata": {
        "id": "qQbm5Wr7azfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [vocab[line] for line in subsampled]"
      ],
      "metadata": {
        "id": "7l6-u1LIay1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get center and context"
      ],
      "metadata": {
        "id": "X7hGvAH0bNKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_centers_and_contexts(corpus, max_window_size):\n",
        "    \"\"\"Return center words and context words in skip-gram.\"\"\"\n",
        "    centers, contexts = [], []\n",
        "    for line in corpus:\n",
        "        # To form a \"center word--context word\" pair, each sentence needs to\n",
        "        # have at least 2 words\n",
        "        if len(line) < 2:\n",
        "            continue\n",
        "        centers += line\n",
        "        for i in range(len(line)):  # Context window centered at `i`\n",
        "            window_size = random.randint(1, max_window_size)\n",
        "            indices = list(range(max(0, i - window_size),\n",
        "                                 min(len(line), i + 1 + window_size)))\n",
        "            # Exclude the center word from the context words\n",
        "            indices.remove(i)\n",
        "            contexts.append([line[idx] for idx in indices])\n",
        "    return centers, contexts"
      ],
      "metadata": {
        "id": "KoW6-4_ya6Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomGenerator:\n",
        "    \"\"\"Randomly draw among {1, ..., n} according to n sampling weights.\"\"\"\n",
        "    def __init__(self, sampling_weights):\n",
        "        # Exclude\n",
        "        self.population = list(range(1, len(sampling_weights) + 1))\n",
        "        self.sampling_weights = sampling_weights\n",
        "        self.candidates = []\n",
        "        self.i = 0\n",
        "\n",
        "    def draw(self):\n",
        "        if self.i == len(self.candidates):\n",
        "            # Cache `k` random sampling results\n",
        "            self.candidates = random.choices(\n",
        "                self.population, self.sampling_weights, k=10000)\n",
        "            self.i = 0\n",
        "        self.i += 1\n",
        "        return self.candidates[self.i - 1]\n"
      ],
      "metadata": {
        "id": "aCCxOJLabE6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get negatives (SG)"
      ],
      "metadata": {
        "id": "1OMxH1iMbVF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_negatives(all_contexts, vocab, counter, K):\n",
        "    \"\"\"Return noise words in negative sampling.\"\"\"\n",
        "    # Sampling weights for words with indices 1, 2, ... (index 0 is the\n",
        "    # excluded unknown token) in the vocabulary\n",
        "    sampling_weights = [counter[vocab.to_tokens(i)]**0.75\n",
        "                        for i in range(1, len(vocab))]\n",
        "    all_negatives, generator = [], RandomGenerator(sampling_weights)\n",
        "    for contexts in all_contexts:\n",
        "        negatives = []\n",
        "        while len(negatives) < len(contexts) * K:\n",
        "            neg = generator.draw()\n",
        "            # Noise words cannot be context words\n",
        "            if neg not in contexts:\n",
        "                negatives.append(neg)\n",
        "        all_negatives.append(negatives)\n",
        "    return all_negatives\n",
        "\n",
        "all_negatives = get_negatives(all_contexts, vocab, counter, 5)"
      ],
      "metadata": {
        "id": "pcH0h7MQbKcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get negatives (CBOW)"
      ],
      "metadata": {
        "id": "F6NKyvqKdwhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_negatives_cbow(all_centers, vocab, counter, K):\n",
        "    \"\"\"Return noise words in negative sampling.\"\"\"\n",
        "    # Sampling weights for words with indices 1, 2, ... (index 0 is the\n",
        "    # excluded unknown token) in the vocabulary\n",
        "    sampling_weights = [counter[vocab.to_tokens(i)]**0.75\n",
        "                        for i in range(1, len(vocab))]\n",
        "    all_negatives, generator = [], RandomGenerator(sampling_weights)\n",
        "    for center in all_centers:\n",
        "        negatives = []\n",
        "        while len(negatives) < K:\n",
        "            neg = generator.draw()\n",
        "            # Noise words cannot be context words\n",
        "            if neg != center:\n",
        "                negatives.append(neg)\n",
        "        all_negatives.append(negatives)\n",
        "    return all_negatives\n",
        "\n",
        "all_negatives_cbow = get_negatives_cbow(all_centers, vocab, counter, 5)\n",
        "print(len(all_negatives_cbow))"
      ],
      "metadata": {
        "id": "rspxxRF_dynU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batchify(data):\n",
        "    \"\"\"Return a minibatch of examples for skip-gram with negative sampling.\"\"\"\n",
        "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
        "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
        "    for center, context, negative in data:\n",
        "        cur_len = len(context) + len(negative)\n",
        "        centers += [center]\n",
        "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
        "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
        "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
        "    return (torch.tensor(centers).reshape((-1, 1)), torch.tensor(\n",
        "        contexts_negatives), torch.tensor(masks), torch.tensor(labels))\n"
      ],
      "metadata": {
        "id": "j1duZVwobbJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_ptb(batch_size, max_window_size, num_noise_words):\n",
        "    \"\"\"Download the PTB dataset and then load it into memory.\"\"\"\n",
        "    num_workers = d2l.get_dataloader_workers()\n",
        "    sentences = read_ptb()\n",
        "    vocab = d2l.Vocab(sentences, min_freq=10)\n",
        "    subsampled, counter = subsample(sentences, vocab)\n",
        "    corpus = [vocab[line] for line in subsampled]\n",
        "    all_centers, all_contexts = get_centers_and_contexts(\n",
        "        corpus, max_window_size)\n",
        "    all_negatives = get_negatives(\n",
        "        all_contexts, vocab, counter, num_noise_words)\n",
        "\n",
        "    class PTBDataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, centers, contexts, negatives):\n",
        "            assert len(centers) == len(contexts) == len(negatives)\n",
        "            self.centers = centers\n",
        "            self.contexts = contexts\n",
        "            self.negatives = negatives\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            return (self.centers[index], self.contexts[index],\n",
        "                    self.negatives[index])\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.centers)\n",
        "\n",
        "    dataset = PTBDataset(all_centers, all_contexts, all_negatives)\n",
        "\n",
        "    data_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True,\n",
        "                                      collate_fn=batchify,\n",
        "                                      num_workers=num_workers)\n",
        "    return data_iter, vocab"
      ],
      "metadata": {
        "id": "Jr4wI3MwbdMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, max_window_size, num_noise_words = 512, 5, 5\n",
        "data_iter, vocab = d2l.load_data_ptb(batch_size, max_window_size,\n",
        "                                     num_noise_words)"
      ],
      "metadata": {
        "id": "DoDuD98Zbil_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, max_window_size, num_noise_words = 512, 5, 5\n",
        "data_iter, vocab = d2l.load_data_ptb(batch_size, max_window_size,\n",
        "                                     num_noise_words)"
      ],
      "metadata": {
        "id": "QMrRDvKFbkew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Negative sampling (SG)"
      ],
      "metadata": {
        "id": "YIVZ15SPcEop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
        "    v = embed_v(center)\n",
        "    u = embed_u(contexts_and_negatives)\n",
        "    pred = torch.bmm(v, u.permute(0, 2, 1))\n",
        "    return pred"
      ],
      "metadata": {
        "id": "b9hKImNObn0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skip_gram(torch.ones((2, 1), dtype=torch.long),\n",
        "          torch.ones((2, 4), dtype=torch.long), embed, embed).shape\n"
      ],
      "metadata": {
        "id": "XhHwsLEabvMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SigmoidBCELoss(nn.Module):\n",
        "    # Binary cross-entropy loss with masking\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, inputs, target, mask=None):\n",
        "        out = nn.functional.binary_cross_entropy_with_logits(\n",
        "            inputs, target, weight=mask, reduction=\"none\")\n",
        "        return out.mean(dim=1)\n",
        "\n",
        "loss = SigmoidBCELoss()"
      ],
      "metadata": {
        "id": "vUvQj37Ebw-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = torch.tensor([[1.1, -2.2, 3.3, -4.4]] * 2)\n",
        "label = torch.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]])\n",
        "mask = torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0]])\n",
        "loss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1)"
      ],
      "metadata": {
        "id": "a2TXaNlLbzPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmd(x):\n",
        "    return -math.log(1 / (1 + math.exp(-x)))\n",
        "\n",
        "print(f'{(sigmd(1.1) + sigmd(2.2) + sigmd(-3.3) + sigmd(4.4)) / 4:.4f}')\n",
        "print(f'{(sigmd(-1.1) + sigmd(-2.2)) / 2:.4f}')\n"
      ],
      "metadata": {
        "id": "dEJqVIi8b35l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, data_iter, lr, num_epochs, device=d2l.try_gpu()):\n",
        "    def init_weights(module):\n",
        "        if type(module) == nn.Embedding:\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "    net.apply(init_weights)\n",
        "    net = net.to(device)\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
        "                            xlim=[1, num_epochs])\n",
        "    # Sum of normalized losses, no. of normalized losses\n",
        "    metric = d2l.Accumulator(2)\n",
        "    for epoch in range(num_epochs):\n",
        "        timer, num_batches = d2l.Timer(), len(data_iter)\n",
        "        for i, batch in enumerate(data_iter):\n",
        "            optimizer.zero_grad()\n",
        "            center, context_negative, mask, label = [\n",
        "                data.to(device) for data in batch]\n",
        "\n",
        "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
        "            l = (loss(pred.reshape(label.shape).float(), label.float(), mask)\n",
        "                     / mask.sum(axis=1) * mask.shape[1])\n",
        "            l.sum().backward()\n",
        "            optimizer.step()\n",
        "            metric.add(l.sum(), l.numel())\n",
        "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
        "                animator.add(epoch + (i + 1) / num_batches,\n",
        "                             (metric[0] / metric[1],))\n",
        "    print(f'loss {metric[0] / metric[1]:.3f}, '\n",
        "          f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')"
      ],
      "metadata": {
        "id": "Xtge-N7Vb6hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr, num_epochs = 0.002, 5\n",
        "train(net, data_iter, lr, num_epochs)"
      ],
      "metadata": {
        "id": "TahVooOBb-Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Negative sampling (CBOW) - not test yet"
      ],
      "metadata": {
        "id": "JpBfUKgjizJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cbow(all_centers_and_negatives, contexts, embed_v, embed_u):\n",
        "    \"\"\"\n",
        "    all_centers_and_negatives: (batch, 1 + K)\n",
        "    contexts: (batch, num_contexts)\n",
        "    \"\"\"\n",
        "    # 1. Get context embeddings and average them (input embeddings)\n",
        "    v = embed_v(contexts).mean(dim=1, keepdim=True)     # (batch, 1, embed_size)\n",
        "\n",
        "    # 2. Get embeddings for positive center + negatives (output embeddings)\n",
        "    u = embed_u(all_centers_and_negatives)              # (batch, 1+K, embed_size)\n",
        "\n",
        "    # 3. Dot product â†’ logits for BCE loss\n",
        "    pred = torch.bmm(v, u.permute(0, 2, 1))             # (batch, 1, 1+K)\n",
        "\n",
        "    return pred"
      ],
      "metadata": {
        "id": "zqBbs8Nxi0rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cbow(net, data_iter, lr, num_epochs, device=d2l.try_gpu()):\n",
        "    def init_weights(module):\n",
        "        if type(module) == nn.Embedding:\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "\n",
        "    net.apply(init_weights)\n",
        "    net = net.to(device)\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
        "                            xlim=[1, num_epochs])\n",
        "\n",
        "    metric = d2l.Accumulator(2)  # sum of loss, count of losses\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        timer, num_batches = d2l.Timer(), len(data_iter)\n",
        "\n",
        "        for i, batch in enumerate(data_iter):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Batch = (contexts, centers_negatives, mask, label)\n",
        "            contexts, centers_negatives, mask, label = [\n",
        "                data.to(device) for data in batch\n",
        "            ]\n",
        "\n",
        "            # Call CBOW\n",
        "            pred = cbow(centers_negatives, contexts, net[0], net[1])\n",
        "\n",
        "            # Shape match for BCE loss\n",
        "            l = (loss(pred.reshape(label.shape).float(),\n",
        "                      label.float(),\n",
        "                      mask)\n",
        "                 / mask.sum(axis=1) * mask.shape[1])\n",
        "\n",
        "            l.sum().backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            metric.add(l.sum(), l.numel())\n",
        "\n",
        "            # Update animator\n",
        "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
        "                animator.add(epoch + (i + 1) / num_batches,\n",
        "                             (metric[0] / metric[1],))\n",
        "\n",
        "        print(f'epoch {epoch+1}: loss {metric[0] / metric[1]:.3f}, '\n",
        "              f'{metric[1] / timer.stop():.1f} tokens/sec on {device}')\n"
      ],
      "metadata": {
        "id": "n_2nZo-mjwYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical Softmax (SG)"
      ],
      "metadata": {
        "id": "YXwT3DkJcMIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HuffmanNode:\n",
        "    def __init__(self, freq, idx=None, left=None, right=None):\n",
        "        self.freq = freq\n",
        "        self.idx = idx # Word index (if leaf) or internal node index\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        return self.freq < other.freq"
      ],
      "metadata": {
        "id": "ByYvq-ntcQLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_huffman_tree(word_freq, vocab_size):\n",
        "    # Create leaf nodes for words in the vocabulary (indices 0 -> vocab_size - 1)\n",
        "    heap = [HuffmanNode(freq, idx=i) for i, (word, freq) in enumerate(word_freq.items())]\n",
        "    heapq.heapify(heap)\n",
        "\n",
        "    # Start indexing internal nodes starting from vocab_size\n",
        "    internal_node_idx = vocab_size\n",
        "\n",
        "    while len(heap) > 1:\n",
        "        left = heapq.heappop(heap)\n",
        "        right = heapq.heappop(heap)\n",
        "\n",
        "        # Create a new parent node with the next available index\n",
        "        merged = HuffmanNode(left.freq + right.freq, idx=internal_node_idx, left=left, right=right)\n",
        "        internal_node_idx += 1 # Increment index for the next internal node\n",
        "\n",
        "        heapq.heappush(heap, merged)\n",
        "\n",
        "    return heap[0] # Return root"
      ],
      "metadata": {
        "id": "BANou4wvc02I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_codes(node, code=\"\", path=[], codes={}, paths={}):\n",
        "    if node.left is None and node.right is None: # Is a leaf node\n",
        "        codes[node.idx] = code\n",
        "        paths[node.idx] = path # Store list of parent node indices\n",
        "    else:\n",
        "        # When traversing down, add current node index to the path\n",
        "        new_path = path + [node.idx]\n",
        "        assign_codes(node.left, code + \"0\", new_path, codes, paths)\n",
        "        assign_codes(node.right, code + \"1\", new_path, codes, paths)\n",
        "    return codes, paths"
      ],
      "metadata": {
        "id": "iSfL0N70c27z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pairs_from_centers_and_contexts(corpus):\n",
        "  pairs = []\n",
        "  assert len(all_centers) == len(all_contexts)\n",
        "  for i in range(len(all_centers)):\n",
        "    pairs.append((all_centers[i], all_contexts[i]))\n",
        "  return pairs"
      ],
      "metadata": {
        "id": "L3MVJMXZc6jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = build_huffman_tree(counter, len(vocab))\n",
        "codes, paths = assign_codes(root)\n",
        "pairs = generate_pairs_from_centers_and_contexts(corpus)"
      ],
      "metadata": {
        "id": "diRT0ffAc8P0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGHierarchicalSoftmax(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        # Embedding for context words\n",
        "        self.context_embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "        # Embedding for Huffman tree nodes\n",
        "        self.node_embeddings = nn.Embedding(2 * vocab_size, embed_size)\n",
        "\n",
        "    def forward(self, context_idx, path_indices, code_bits):\n",
        "        # 1. Calculate hidden vector h by averaging context vectors\n",
        "        # context_idx: [batch_size, context_window * 2] -> here batch=1 so it is [len_context]\n",
        "        embeds = self.context_embeddings(context_idx) # [len_context, embed_size]\n",
        "        h = embeds.mean(dim=0).unsqueeze(0) # [1, embed_size]\n",
        "\n",
        "        # 2. Get embeddings of nodes along the path\n",
        "        path_tensor = torch.tensor(path_indices, dtype=torch.long)\n",
        "        node_embeds = self.node_embeddings(path_tensor) # [path_len, embed_size]\n",
        "\n",
        "        # 3. Calculate Dot product\n",
        "        # [1, embed_size] * [embed_size, path_len] -> [1, path_len]\n",
        "        scores = torch.sigmoid(torch.matmul(h, node_embeds.t())).squeeze()\n",
        "\n",
        "        loss = 0\n",
        "        epsilon = 1e-9 # CRITICAL: Prevent log(0) -> NaN\n",
        "\n",
        "        # 4. Calculate Loss based on Huffman codes\n",
        "        for i, bit in enumerate(code_bits):\n",
        "            # Handle case where path length is 1 (scalar score)\n",
        "            score = scores[i] if len(scores.shape) > 0 else scores\n",
        "\n",
        "            if bit == \"1\":\n",
        "                loss += -torch.log(score + epsilon)\n",
        "            else:\n",
        "                loss += -torch.log(1 - score + epsilon)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "kghvscNjdB2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot utils"
      ],
      "metadata": {
        "id": "JyOwvm7rdL7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(losses, title, filename):\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(range(len(losses)), losses)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.savefig(f'outputs/{filename}')\n",
        "    print(f\"Loss chart saved at outputs/{filename}\")\n",
        "\n",
        "def visualize_embeddings(embeddings, vocab, filename):\n",
        "    from sklearn.manifold import TSNE\n",
        "    import matplotlib.pyplot as plt\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "    emb_2d = tsne.fit_transform(embeddings)\n",
        "    plt.figure(figsize=(10,8))\n",
        "    for i, word in enumerate(vocab):\n",
        "        plt.scatter(emb_2d[i,0], emb_2d[i,1])\n",
        "        plt.annotate(word, (emb_2d[i,0], emb_2d[i,1]))\n",
        "    plt.title('t-SNE Visualization of Embeddings')\n",
        "    plt.savefig(f'outputs/{filename}')\n",
        "    print(f\"Embedding visualization saved at outputs/{filename}\")"
      ],
      "metadata": {
        "id": "NB1y2UOvdRCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Hierarchical Softmax (SG)"
      ],
      "metadata": {
        "id": "prIU8VD2dYfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 50 # Demo embedding size\n",
        "lr = 0.001\n",
        "epochs = 5\n",
        "\n",
        "model = SGHierarchicalSoftmax(len(vocab), embed_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "losses = []\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"Start Training...\")\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    # Run first 5000 pairs for testing; remove [:5000] for full dataset run\n",
        "    for i, (center_idx, context_ids) in enumerate(pairs[:5000]):\n",
        "        # Skip if context word is not in vocab (rare words that might have been pruned)\n",
        "        context_idx = torch.tensor([context_id for context_id in context_ids], dtype=torch.long)\n",
        "        if center_idx not in paths: continue\n",
        "\n",
        "        # Retrieve pre-calculated path and codes\n",
        "        path_nodes = paths[center_idx] # List of integers\n",
        "        code_bits = codes[center_idx]  # String \"010...\"\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(context_idx, path_nodes, code_bits)\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients to prevent exploding gradients (optional but recommended)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    losses.append(total_loss)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "# Use plot function if available in utils\n",
        "try:\n",
        "    plot_loss(losses, 'Hierarchical Softmax Loss', 'loss_hierarchical_softmax.png')\n",
        "    visualize_embeddings(model.center_embeddings.weight.detach().numpy(), vocab[:200], 'tsne_hsoftmax.png')\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "wNFc1RcudXbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical Softmax (CBOW) - Not test yet"
      ],
      "metadata": {
        "id": "wvHDvVfydjUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HierarchicalSoftmax(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.center_embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "        # The number of internal nodes in a Huffman tree with N leaves is N-1.\n",
        "        # Total nodes requiring embedding is approx 2*N.\n",
        "        self.node_embeddings = nn.Embedding(2 * vocab_size, embed_size)\n",
        "\n",
        "    def forward(self, center_idx, path_indices, code_bits):\n",
        "        # center_idx: [1]\n",
        "        center_embed = self.center_embeddings(center_idx) # [1, embed_size]\n",
        "\n",
        "        # Convert path_indices to tensor to fetch embeddings in one go (Batch processing)\n",
        "        path_tensor = torch.tensor(path_indices, dtype=torch.long) # [path_len]\n",
        "        node_embeds = self.node_embeddings(path_tensor) # [path_len, embed_size]\n",
        "\n",
        "        # Calculate dot product between center and all nodes in the path\n",
        "        # [1, embed_size] * [embed_size, path_len] -> [1, path_len]\n",
        "        scores = torch.sigmoid(torch.matmul(center_embed, node_embeds.t())).squeeze()\n",
        "\n",
        "        loss = 0\n",
        "        epsilon = 1e-9 # CRITICAL FIX: Avoid log(0) resulting in NaN\n",
        "\n",
        "        # Iterate through bits to calculate loss\n",
        "        for i, bit in enumerate(code_bits):\n",
        "            # Handle cases where path length might be 1 (scalar score)\n",
        "            score = scores[i] if len(scores.shape) > 0 else scores\n",
        "            if bit == \"1\":\n",
        "                loss += -torch.log(score + epsilon)\n",
        "            else:\n",
        "                loss += -torch.log(1 - score + epsilon)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "jm-QZJLIdmYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 50 # Demo embedding size\n",
        "lr = 0.001\n",
        "epochs = 5\n",
        "\n",
        "model = HierarchicalSoftmax(len(vocab), embed_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "losses = []\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"Start Training...\")\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    # Run first 5000 pairs for testing; remove [:5000] for full dataset run\n",
        "    for i, (center, context) in enumerate(pairs[:50000]):\n",
        "        center_idx = torch.tensor([center], dtype=torch.long)\n",
        "        context_idx = context\n",
        "\n",
        "        # Skip if context word is not in vocab (rare words that might have been pruned)\n",
        "        # if context_idx not in paths: continue\n",
        "\n",
        "        # Retrieve pre-calculated path and codes\n",
        "        path_nodes = paths[context_idx] # List of integers\n",
        "        code_bits = codes[context_idx]  # String \"010...\"\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(center_idx, path_nodes, code_bits)\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients to prevent exploding gradients (optional but recommended)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    losses.append(total_loss)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training time: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "# Use plot function if available in utils\n",
        "try:\n",
        "    plot_loss(losses, 'Hierarchical Softmax Loss', 'loss_hierarchical_softmax.png')\n",
        "    visualize_embeddings(model.center_embeddings.weight.detach().numpy(), vocab[:200], 'tsne_hsoftmax.png')\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "9TnVbaH9hOgo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}